<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>IN4391 Distributed Systems on Nick Tehrany</title>
        <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/</link>
        <description>Recent content in IN4391 Distributed Systems on Nick Tehrany</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 07 Feb 2021 08:34:56 -0600</lastBuildDate>
        <atom:link href="https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Big Data and Distributed ML</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/bigdata/</link>
            <pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/bigdata/</guid>
            <description>Three different models of parallel computing:
 Shared memory with multiple CPUs connected to a cache-coherent interconnect to multiple RAM modules, to which all have access. Distributed memory with multiple CPUs all being connected to their own RAM module and a non-cache-coherent interconnect. OpenMP where data is used where the computation is, which is the interface for multi-plaotform shared memory multiprocessing.  Big Data First Generation Distributed File Systems The first open-source system is HDFS, which is a block based file system with built in replication and metadata journaling.</description>
            <content type="html"><![CDATA[<p>Three different models of parallel computing:</p>
<ol>
<li>Shared memory with multiple CPUs connected to a cache-coherent interconnect to multiple RAM modules, to which all have access.</li>
<li>Distributed memory with multiple CPUs all being connected to their own RAM module and a non-cache-coherent interconnect.</li>
<li>OpenMP where data is used where the computation is, which is the interface for multi-plaotform shared memory multiprocessing.</li>
</ol>
<h3 id="big-data">Big Data</h3>
<h4 id="first-generation-distributed-file-systems">First Generation Distributed File Systems</h4>
<p>The first open-source system is HDFS, which is a block based file system with built in replication and metadata journaling. It typically keeps 3 data nodes that receive the writes and the name node receives the addBlock that the client has added a block of data, after which then the data nodes confirm to the name node that they have received the new data.</p>
<p>MapReduce on the other hand works with map workers that apply some mapping function on the data, and the reduce worker that reduce the data of the mappers (e.g. summing counts of words). It first splits the input into chunks, which then go to the mapper. A possible optimization is to pre-reduce data during the map phase to save network bandwidth (this can be done if the function is commutative and associative such as adding to each element in an array individually). Lastly the data of the mappers is shuffled, since the reduce node depends on the data of all the mappers.</p>
<h4 id="second-generation-systems">Second Generation Systems</h4>
<p>Apache Spark is the next gen system that uses Resilient Distributed Datasets (RDD) of which partitions can be kept in memory, and it works on course granular transformations instead of fine grained ones. For this Spark has a master node that splits up the tasks and multiple executor nodes of which one is the driver. Executor nodes are then also used for replication of partitions of the RDDs. RDDs are partitions that are immutable (read-only). However also here the reducer has a wide depency (it depends on all the data of all the mappers). To solve this we can shuffle using a hash function that then maps the result of each map operation on a partition to the same result partition for the reducer. The problem with this is that there are a lot of files for each mapper writing its results to the same partition. To solve this a consolidate hash function is used that creates only a single file for a partition to which the mappers write their result, but this can generate random I/O instead of sequential I/O, which is bad for performance.</p>
<p>To solve this a sortShuffle is used in which partitions are append only maps and as soon as they fill up (in memory) data is spilled to a file. These files are sorted and indexed such that the reducer can then use some sorting algorithm (e.g. minHeapMerge) to reduce based on already sorted values. This way there is only a narrow dependency from the resulting partitions to the data partitions from the mappers. The group by key would still have a wide dependency, therefore Spark uses execution stages which end at shuffle boundaries and allow for full synchronization between stages. Stages themselves will be parallelized.</p>
<h3 id="distributed-ml">Distributed ML</h3>
<p>There are two kinds of parallelism in distributed ML, data parallel in which data is split and trained by the same model in parallel, and model parallel where different models are used on the same data to then combine the model in the end.</p>
<p>Distributed ML can be done centralized with different ML models combining results in some ensembling node which then gives the final trained model, or in a tree structure where data goes to every node, each node sends its gradient to the root of the tree (or its forwarded there across other nodes) and the root combines it and sends it down the tree.</p>
<p>The same can be done with a parameter server (just a kv-store) to which all nodes send their results and at which point the final trained model is computed. Or this can be done fully distributed in a P2P network where nodes inform each other about the training and train based on this.</p>
]]></content>
        </item>
        
        <item>
            <title>Middleware</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/middleware/</link>
            <pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/middleware/</guid>
            <description>Middleware is the part that goes between the OS (or lower level parts) and the applications, these define a set of protocols and APIs between the applications and the network or OS for services and communication. Java RMI is an example of this. Three levels of middleware exist:
 Basic messaging Programming primitives (such as RMI) Full services (platform)  There are different classes of middleware; the remote-call oriented (RPC), the object-oriented (RMI), the service-oriented (CORBA, web services), and the message-oriented (pub/sub, message bus).</description>
            <content type="html"><![CDATA[<p>Middleware is the part that goes between the OS (or lower level parts) and the applications, these define a set of protocols and APIs between the applications and the network or OS for services and communication. Java RMI is an example of this. Three levels of middleware exist:</p>
<ol>
<li>Basic messaging</li>
<li>Programming primitives (such as RMI)</li>
<li>Full services (platform)</li>
</ol>
<p>There are different classes of middleware; the remote-call oriented (RPC), the object-oriented (RMI), the service-oriented (CORBA, web services), and the message-oriented (pub/sub, message bus).</p>
<h3 id="corba">CORBA</h3>
<p>CORBA is a Common Object Request Broker Architecture, that handles interaction between components such as transactions, security, time, etc. across multiple servers that run the client stub for providing an interface to remote calls, and marshalling serialization with the CORBA library to make objects serializable, which all runs on top of the Object Request Broker (ORB).</p>
<h3 id="types-of-middleware">Types of Middleware</h3>
<h4 id="reflection">Reflection</h4>
<p>This method can examine the properties of an object and possibly modify it.</p>
<h4 id="adaption">Adaption</h4>
<p>Adaption can change the behavior of the system, for example at compile-time with macros or at load-time with library interpositioning or at run-time with self modifying code.</p>
]]></content>
        </item>
        
        <item>
            <title>Programming Models and System Architecture</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/systemsarchtecture/</link>
            <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/systemsarchtecture/</guid>
            <description>Design Elements of a Distributed System  The Architecture, is it client-server, hierarchical, peer-to-peer, etc. The programming model that is used, is it remote procedure call (RPC), library based, actor model based (akka), mapreduce, etc.  Tradeoffs between the different programming modles and the sotware architecture have to be made, based on the functional/non-functional requirements, the cost of convenience, which is easier to use based on the hardware for example, and which is needed for a large scale system.</description>
            <content type="html"><![CDATA[<h2 id="design-elements-of-a-distributed-system">Design Elements of a Distributed System</h2>
<ul>
<li>The Architecture, is it client-server, hierarchical, peer-to-peer, etc.</li>
<li>The programming model that is used, is it remote procedure call (RPC), library based, actor model based (akka), mapreduce, etc.</li>
</ul>
<p>Tradeoffs between the different programming modles and the sotware architecture have to be made, based on the functional/non-functional requirements, the cost of convenience, which is easier to use based on the hardware for example, and which is needed for a large scale system.</p>
<h3 id="programming-model">Programming Model</h3>
<p>The programming model is the abstraction of the underlying hardware that forms the execution environment.</p>
<p>These models are often based on multiple abstractions such as the model of computation, communication, or data.</p>
<p>Certain programming models exist for parallel and distributed systems, such as pthread. Different models then offer different functions, one might be synchronous while another is asynchronous.</p>
<p>MPI (message passing interface) is another programming model that offers message passing on point-to-point links or broadcasting.</p>
<h4 id="middleware">Middleware</h4>
<p>Middleware is the part that goes between the OS (or lower level parts) and the applications, these define a set of protocols and APIs between the applications and the network or OS for services and communication. Java RMI is an example of this. Three levels of middleware exist:</p>
<ol>
<li>Basic messaging</li>
<li>Programming primitives (such as RMI)</li>
<li>Full services (platform)</li>
</ol>
<h4 id="interceptors">Interceptors</h4>
<p>Interceptors are software that break the usual execution flow to allow some other application code to be executed. This allows for adaptation of middleware by &ldquo;injecting&rdquo; some additional custom code.</p>
<h4 id="tuple-spaces">Tuple Spaces</h4>
<p>Tuple spaces is another form of a programming model which offers a form of distributed shared memory that is paired with synchronization mechanisms.</p>
<h4 id="actors">Actors</h4>
<p>Actors is another well know model where only an actor can access their internal state and messages can be sent (asynchronously) to other actors. Akka is the Scala example of such a model.</p>
<h3 id="system-architecture">System Architecture</h3>
<p>Pthread implies an implicit system model of MIMD (Multiple Instruction Multiple Data), which is shared memory such that threads can use the same memory.</p>
<p>In large scale systems failures are the norm and will happen.
Message passing with RPC is very tough at a large scale (unless there is a minimal known amount of message passing that only happens between certain edges).</p>
<p>The goal is to restrict the programming modes so that the system can do more automatically (scheduling, data distribution, fault-tolerance, etc.).</p>
<p>Sometimes it is necessary for self adapting systems that change based on some monitoring. Such systems are organized with a feedback-control system and are called  <em>autonomic computing systems</em>.</p>
<h4 id="different-system-architectures">Different System Architectures</h4>
<p>Many different architectures exist:</p>
<ul>
<li>Star (centralized): such as clients only communicating with servers but not with other clients. Or Spark nodes only communicating with the master (though sometimes nodes can communicate with each other)</li>
<li>Hierarchical: DNS is the best example for this, and provides a quicker way to send messages. Imagine peer-to-peer for looking up an IP you would have to possible ask everyone, but with DNS you can find it through tree traversal from the root down.</li>
<li>Super-peers: peers are connected to other peers that have some additional responsiblity (hence super), and these super nodes are then connected to other super nodes. This replaces the need for servers, as it was used by Skype earlier.</li>
<li>All-peers (decentralized): BitTorrent or other peer-to-peer networks.</li>
</ul>
<p>In most cases servers are organized in three tiers, the logical switch (can have multiple servers) that accepts client requests, the application/compute servers that do the actual processing of the request, the the distributed file/database system for storing the data. The logical switch will dispatch the requests to worker servers which use the database systems for retrieving of data.</p>
]]></content>
        </item>
        
        <item>
            <title>Non-Functional Requirements</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/non-functional/</link>
            <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/non-functional/</guid>
            <description>Consistency Client vs. data centric consistency, in the client side model there is a consistency model for every client but not a global consistency. From the perspective of the client there is no difference in the two models. When to use which model is a tradeoff decision again, therefore when you can afford it you want to use a weaker consistency model. The client consistency can be used if there is some notion of a client session.</description>
            <content type="html"><![CDATA[<h3 id="consistency">Consistency</h3>
<p>Client vs. data centric consistency, in the client side model there is a consistency model for every client but not a global consistency. From the perspective of the client there is no difference in the two models. When to use which model is a tradeoff decision again, therefore when you can afford it you want to use a weaker consistency model. The client consistency can be used if there is some notion of a client session.</p>
<p>There are different forms of update propagation such as a notification to invalidate some data, sending new data, or updating data. The type used depends on the complexity of the operations performed.</p>
<p>Sequential vs. causal consistency, sequential consistency execution of everything is done in some sequential order but everyone applies in the same order that everyone agreed upon (it creates a total order of operations), in causal order you reason about which pairs of operations need to be in a certain order, causal is weaker than sequential.</p>
<p>Eventual consistency, things will converge to a consistent state if the writes stop at some point, since consistency is not immediate. Therefore the client can see states that are not directly consistent, as some writes may have not reached all the devices.</p>
<p>Different types of updates:</p>
<ul>
<li>Monotonic reads: accessing the data on different devices</li>
<li>Monotonic writes: updating the data locally and propagating all changes to other servers</li>
</ul>
<p>Different notions in writing $W_2(x_1;x_2))$ reads as the second write happens with $x_2$ being derived from $x_1$, which is a monotonical write (the updates of $x_1$ are propagated to the next server and the next write depends on it). Writing $W_1(x_1|x_2)$ is a write of $x_2$ that is not derived from $x_1$ therefore a read from $x_2$ would be a valid monotonical read.</p>
<h3 id="metrics">Metrics</h3>
<p>As most systems use a queue for storing tasks before they can be executed, queuing theory can be used for measuring the performance of a system with the arrival time, start time, end time, wait time, etc.</p>
<p>One of the modern metrics is elasticity which is the degree to which a system adapts to workload changes that adapt resources dynamically depending on the needs.</p>
<h3 id="gustafsons-law-of-weak-scaling">Gustafson&rsquo;s Law of Weak Scaling</h3>
<p>For any problem that we look at, we look at the relationship between smaller and larger jobs, the communication overhead is almost the same, therefore if we exploit more parallelism for larger jobs, the sequential parts stay constant and additional resources are only used for larger jobs and the speedup comes from the sequential portion.</p>
<h3 id="evaluating-performance-metrics">Evaluating Performance Metrics</h3>
<ul>
<li>Micro-Benchmark: test individual components of a system such as perf or stream</li>
<li>(Micro-)Kernel Benchmark: benchmark hot functions of a system such as Berkeley dwarfs</li>
<li>Synthetic workloads: approximate a model of the behavior of a system with made up scenarios that aim to represent real world workloads such as application skeletons</li>
<li>Real-world workloads: Model the behavior of real word scenarios on systems such as TPC-DS</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Resource Management and Scheduling</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/management/</link>
            <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/management/</guid>
            <description>Reading Material Before Lecture Google Borg Google Borg is a management system for clusters that run several hundreds of thousands of jobs from many different applications, on many different clusters with each having several thousands machines.
Resource Management in Cloud Clouds contain large quantities of shared resources that provide these to large pools of data and computing with a variety of different interfaces. An important part of cloud computing is resource management, as there only are finite resources available.</description>
            <content type="html"><![CDATA[<h2 id="reading-material-before-lecture">Reading Material Before Lecture</h2>
<h3 id="google-borg">Google Borg</h3>
<p>Google Borg is a management system for clusters that run several hundreds of thousands of jobs from many different applications, on many different clusters with each having several thousands machines.</p>
<h3 id="resource-management-in-cloud">Resource Management in Cloud</h3>
<p>Clouds contain large quantities of shared resources that provide these to large pools of data and computing  with a variety of different interfaces. An important part of cloud computing is resource management, as there only are finite resources available.</p>
<h3 id="slurm">Slurm</h3>
<p>Slurm is a cluster management and scheduling system for Linux clusters that are fault-tolerant and highly scalable, and it is open source.</p>
<h3 id="energy-efficient-resource-management">Energy Efficient Resource Management</h3>
<p>In order to fulfill the needs of demand on computational resources, cloud providers have to implement energy efficient hardware and find the tradeoff between performance and energy efficiency, in order to not use enormous amounts of energy, while still meeting the quality of service (QoS) requirements.</p>
<h2 id="lecture-notes">Lecture Notes</h2>
<h3 id="scheduling">Scheduling</h3>
<p>The goal of scheduling is to achieve some form of mutual agreement between the resource providers (operators) and consumers (e.g clients). It also aims to maximize the resource utilization and hence will schedule workloads in a certain way.</p>
<p>The scheduling <strong>mechanism</strong> explains what to do, (i.e. use this policy for making a decision), and the scheduling <strong>policy</strong> explains how to do something (schedule with FIFO or randomly).</p>
<p>There are many different scheduling policies such as FIFO (does not always achieve minimal avg RT: average return time that is the addition of the return times of all tasks divided by the number of tasks), or shortest job first (suffers from starvation if a continuous stream of short jobs get scheduled when a long job is waiting to be run), or more advanced policies such as time slicing, which is used in OS, gives jobs a certain time slice and interrupts them once the time is done. Time slices can be assigned in round robin fashion. There are other techniques such as proportional share (jobs get resources proportional to what they require). Time slicing is efficient in OS but not for cluster scheduling.</p>
<p>The scheduler has the jobs to manage the environment, goals and other metrics for optimization, and based on the information schedule incoming workloads efficiently. Typically schedulers consume 5% of the cycles in datacenters (relatively low overhead considering the overhead of more advanced optimizations).</p>
<h3 id="portfolio-scheduling">Portfolio Scheduling</h3>
<p>Since datacenters cannot work without a scheduler, here is proposed to use a set of schedulers, and apply some scheduler for some period, analyze its performance, and then use the next scheduler and repeat this.</p>
<h3 id="types-of-jobs-in-ds">Types of Jobs in DS</h3>
<ul>
<li>Parallel jobs: high-performance computing</li>
<li>Bags-of-tasks: bag tasks together (e.g. parameter sweep computing (PSC), high throughput computing)</li>
<li>Workflows: MapReduce, event streaming and applying filters</li>
</ul>
<h3 id="architecture-for-distributed-management">Architecture for Distributed Management</h3>
<p>The goal is to execute the full workloads while ensuring the service level agreement (SLA) and objectives (SLO) are met. The issues come from how resources are allocated, the complexity of the software that manages the architecture, load imbalance of different machines being used much more than others, having central points (single point of failure), and having the ability to scale significantly.</p>
<p>Single Cluster Architecture: Uses a single headnode and several nodes to which the headnode then assigns jobs (master and worker nodes).</p>
]]></content>
        </item>
        
        <item>
            <title>Functional Requirements</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/functionalrequirements/</link>
            <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/functionalrequirements/</guid>
            <description>Reading Material Before Lecture Four Types of Architecture  Layered: where each part is stacked on top of each other and communicates down and up the stack only (e.g. TCP/IP stack) Object: where each object can communicate with other objects using remote method calls. Event based: where there is one message bus that connects the different components and each component can then publish events on the message bus. Shared data: where there is some shared (persistent) data space to which components can publish data or request data from.</description>
            <content type="html"><![CDATA[<h2 id="reading-material-before-lecture">Reading Material Before Lecture</h2>
<h3 id="four-types-of-architecture">Four Types of Architecture</h3>
<ol>
<li>Layered: where each part is stacked on top of each other and communicates down and up the stack only (e.g. TCP/IP stack)</li>
<li>Object: where each object can communicate with other objects using remote method calls.</li>
<li>Event based: where there is one message bus that connects the different components and each component can then publish events on the message bus.</li>
<li>Shared data: where there is some shared (persistent) data space to which components can publish data or request data from.</li>
</ol>
<h3 id="remote-procedure-call-rpc">Remote Procedure Call (RPC)</h3>
<p>RPC allows one program to call a procedure (subroutine or method) in another address space than its own, typically on another system, without the programmer explicitly implementing this remote interaction but instead it is the same as if a local process would invoke some method, except now it comes from a non-local process.</p>
<h3 id="message-bus">Message Bus</h3>
<p>Since distributed systems rely on message passing between different systems for sharing of data, the message bus enables this with the addition of hotplugging systems to this at any point in time without affecting any other system.</p>
<h3 id="lamport-timestamps">Lamport Timestamps</h3>
<p>In order to provide ordering on messages in distributed systems, since different nodes are not synchronized, Lamport timestamps can be used to timestamp events with a simple and lightweight algorithm.</p>
<h3 id="vector-clocks">Vector Clocks</h3>
<p>Vector clocks are a more advanced algorithm for generating causal ordering (adhering to the HB relation) in which each message contains the state of the logical clock of the sender. This is an array the size of the number of processes (an index for each process) of clocks for the state of each process.</p>
<p>The difference between Lamport and Vector clocks is that Lamport is only a single value (single clock) whereas Vector is a collection of clocks of all processes in the system.</p>
<h3 id="flat-naming">Flat Naming</h3>
<p>It is vital for distributed systems to have some efficient naming scheme for the different systems/nodes, which is used as their identifier or to determine their location and much more. With flat naming, the identifier is just a set of bits.</p>
<h3 id="hierarchical-naming">Hierarchical Naming</h3>
<p>With hierarchical naming the system or network is divided into subparts of one domain with a single top-level (root) domain and ever expanding sub-domains until there are some leaf nodes.</p>
<h3 id="domain-name-system-dns">Domain Name System (DNS)</h3>
<p>This is a hierarchical naming system for decentralized systems. It effectively implements efficient naming and allows convenient partitioning, which in turn allows desired allocation, i.e. many nodes of the leaf domain and only a few top-level domain nodes.</p>
<h3 id="lightweight-directory-access-protocol-ldap">Lightweight Directory Access Protocol (LDAP)</h3>
<p>LDAP is a directory service similar to flat naming in order to provide lightweight directory naming by associating a record in a directory entry with an associated attribute such that each record is represented as a (attribute, value) pair.</p>
<h2 id="lecture-notes">Lecture Notes</h2>
<h3 id="cluster-vs-grid-computing">Cluster vs Grid computing</h3>
<p>In cluster computing much of the system is the same, they all have homogeneous hardware are connected over some high speed network. With grid computing resources are heterogeneous.</p>
<p>Grids are then connected in a layered architecture of 4 layers (connective and resource are on the same level)</p>
<ol>
<li>Fabric layer: lowest level that provides the interfaces for resources</li>
<li>Connectivity layer: layer that provides the communication between the resources</li>
<li>Resource layer: layer that is responsible for managing the resources</li>
<li>Collective layer: deals with handling the access to the resources such as scheduling, allocation, etc.</li>
<li>Application: application that utilizes the grid</li>
</ol>
<h3 id="raft">Raft</h3>
<p><strong>Safety property:</strong> when there are no byzantine failures there should never be an incorrect result.</p>
<p><strong>Availability:</strong>  $n/2+1$ servers required to produce valid result.</p>
<p><strong>No clocks:</strong> Nodes do not depend on clocks.</p>
<p><strong>Stragglers:</strong> It can handle stragglers (nodes making progress very slowly) if $N/2+1$ servers vote in the end.</p>
<p>Leaders are elected for each term and each leader keeps track of the current term number, if election failed the term has no leader.</p>
<h3 id="consensus">Consensus</h3>
<p>A correct consensus protocol needs to fulfill:</p>
<ol>
<li>Termination: At some point every correct process needs to decide on some value.</li>
<li>Integrity: If all correct processes propose some value $v$ then the final result must be that value $v$.</li>
<li>Agreement: Every correct process has to agree on the same value.</li>
</ol>
<h3 id="replication">Replication</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Reliability/Redundancy</li>
<li>geographical replication and more nodes to handle client requests</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Lower performance due to the need to maintain consistency across all servers (CAP theorem, atomic operations difficult on many servers)</li>
<li>Consistency also uses up resources (to manage consistency, i.e. SMR protocols)</li>
</ul>
<p>Amdahl&rsquo;s Law that is given as</p>
<p>$$S(n)=\frac{1}{(1-P)+\frac{P}{n}}$$</p>
<p>stating that speedup of an application is a function of how parallelizable the application is, throwing unlimited resources at it will at some point not lead to any performance gains.</p>
<p>Replication systems have <em>Leaders</em> or <em>Masters</em>, which are the servers that take the client requests, and <em>Followers</em>, <em>slaves</em>, or <em>Replicas</em> that provide the read only data access. Systems can have a single leader (master-slave configuration), multiple leaders (master-master configuration) or be leaderless replication where all nodes are the same.</p>
<p>When the system is synchronous the writes need to be configured by a specified number of slaves before it can be considered successful. In an asynchronous system the master can classify it as successful immediately and the slaves apply changes when they are ready.</p>
<p><strong>Push-based</strong> systems are where servers provide the modifications to the slaves, whereas <strong>pull-based</strong> is where clients are asking for updates. <strong>Leases</strong> allow for some time the server will send updates but once the lease expires the client will have to start pulling or renew the lease to get the updates from the master. Leases can be age-based, renewal-frequency based (clients that request more often get longer leases), or overhead based (busy servers give shorter leases).</p>
<h3 id="consistency">Consistency</h3>
<p><strong>A</strong>tomicity: Updates are either done full or not at all.</p>
<p><strong>C</strong>onsistency: The system will always be in a valid state, transitions are only from valid to valid.</p>
<p><strong>I</strong>solation: Transactions do not interfere with each other.</p>
<p><strong>D</strong>urability: Modifications of successfully commited transactions are never lost.</p>
<p>Two schedules are <strong>Equivalence schedules</strong> if they work on the same transactions and the conflict pairs are ordered in the same way. A schedule is then considered to be correct if it is equivalent to a serializable schedule and hence is also serializable. Serializabily differs from linearizability in that it works on multiple objects in arbitrary order while linearizability works on a single object in real time order.</p>
<p>This can be achieved by using <strong>Two-Phase Locking</strong> which means that in order to participate in a transaction it needs to acquire all necessary locks for that object (rule 1), and once a transaction releases a lock it cannot acquire any more locks (rule 2).</p>
<p>In a distributed system this can be achieved with <strong>Two-Phase Commit (2PC)</strong> where the coordinator sends a <em>VOTE_REQ</em> to all slaves, and they respond accordingly <em>VOTE_COMMIT</em> or <em>VOTE_ABORT</em> depending on if the slave succeeded with the transaction, and if the coordinator receives a single abort he sends a <em>GLOBAL_ABORT</em> to all slaves to abort everything, otherwise if he receives no abort he sends a <em>GLOBAL_COMMIT</em> to all the slaves.</p>
]]></content>
        </item>
        
        <item>
            <title>Introduction</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/introduction/</link>
            <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/introduction/</guid>
            <description>What is the difference between parallel computing and distributed computing?
In parallel computing there typically is one job that is split into multiple tasks, which are then split up to different cores/threads that run simultaneously. Such systems typically are homogeneous, the hardware is all the same, i.e. the threads run in the same system on the same CPU.
In distributed computing there are also multiple tasks, but these can come from one job or possibly multiple jobs.</description>
            <content type="html"><![CDATA[<p>What is the difference between parallel computing and distributed computing?</p>
<p>In parallel computing there typically is one job that is split into multiple tasks, which are then split up to different cores/threads that run simultaneously. Such systems typically are homogeneous, the hardware is all the same, i.e. the threads run in the same system on the same CPU.</p>
<p>In distributed computing there are also multiple tasks, but these can come from one job or possibly multiple jobs. These tasks run on heterogeneous hardware, different computers in different locations with different specifications, and hence requires some synchronization to keep the overall system in a legal state.</p>
<p>A distributed system is a system that consists of a colleciton of independent computers/servers that appear to the end user (client) as a single coherent system. A distributed system should make resources readily available and hide from the client that it is distributed (be transparent) and it should be scalable.</p>
<p>Problems arise due to the need for synchronization, which is achieved by message exchanging, which is far from perfect. Messages can be lost, corrupted, or delayed, thus the communication is asynchronous.</p>
<p>Distributed systems can run in a shared state, shared memory, or not in a shared state, where there is no notion of a common clock and all communication happens with messages. Distributed systems then have the following properties</p>
<ul>
<li><strong>C</strong>onsistency: stating that there exists one copy of the data that is up-to-date</li>
<li><strong>A</strong>vailability: stating that it always has to be possible to modify or retrieve some data</li>
<li><strong>P</strong>artition Resilience: stating that if the network were to be interrupted, the system can handle this</li>
</ul>
<p>An example of a distributed system is a distributed hash table (DHT), where individual nodes have finger tables such that the number of nodes to send a message to for finding the successor is in the order of $O(\text{log }n)$. If nodes fail, others can take over since data is replicated over different nodes. But this fails with network partitions as it would create two different rings.</p>
<p>Hence when designing distributed systems it is about finding the tradeoff of the three properties for the requirements of the system.</p>
<p>Another distributed system is a data cache that keeps time stamped information and a secondary engine then runs, which is permanently active, to retrieve the information from the system and update the cache.</p>
]]></content>
        </item>
        
        <item>
            <title>Info</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/info/</link>
            <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/info/</guid>
            <description>IN4391 Distributed Systems notes at TUDelft (Q3).
Studyguide: https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55274 Book: Andrew S. Tanenbaum, Maarten Van Steen, Distributed Systems, Principles and Paradigms (2nd Edition), Prentice Hall, 2006.
The content is based on the lecture material and summarized by myself, due to licensing no actual content of the course is copied here and any images and phrases used come from the book.</description>
            <content type="html"><![CDATA[<p>IN4391 Distributed Systems notes at TUDelft (Q3).</p>
<p>Studyguide: <a href="https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55274">https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55274</a>
Book: Andrew S. Tanenbaum, Maarten Van Steen, Distributed Systems, Principles and Paradigms (2nd Edition), Prentice Hall, 2006.</p>
<p>The content is based on the lecture material and summarized by myself, due to licensing no actual content of the course is copied here and any images and phrases used come from the book.</p>
]]></content>
        </item>
        
    </channel>
</rss>
