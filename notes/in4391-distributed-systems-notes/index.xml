<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>IN4391 Distributed Systems on Nick Tehrany</title>
        <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/</link>
        <description>Recent content in IN4391 Distributed Systems on Nick Tehrany</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 07 Feb 2021 08:34:56 -0600</lastBuildDate>
        <atom:link href="https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Middleware</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/middleware/</link>
            <pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/middleware/</guid>
            <description>Middleware Middleware is the part that goes between the OS (or lower level parts) and the applications, these define a set of protocols and APIs between the applications and the network or OS for services and communication. Java RMI is an example of this. Three levels of middleware exist:
 Basic messaging Programming primitives (such as RMI) Full services (platform)  There are different classes of middleware; the remote-call oriented (RPC), the object-oriented (RMI), the service-oriented (CORBA, web services), and the message-oriented (pub/sub, message bus).</description>
            <content type="html"><![CDATA[<h2 id="middleware">Middleware</h2>
<p>Middleware is the part that goes between the OS (or lower level parts) and the applications, these define a set of protocols and APIs between the applications and the network or OS for services and communication. Java RMI is an example of this. Three levels of middleware exist:</p>
<ol>
<li>Basic messaging</li>
<li>Programming primitives (such as RMI)</li>
<li>Full services (platform)</li>
</ol>
<p>There are different classes of middleware; the remote-call oriented (RPC), the object-oriented (RMI), the service-oriented (CORBA, web services), and the message-oriented (pub/sub, message bus).</p>
<h3 id="corba">CORBA</h3>
<p>CORBA is a Common Object Request Broker Architecture, that handles interaction between components such as transactions, security, time, etc. across multiple servers that run the client stub for providing an interface to remote calls, and marshalling serialization with the CORBA library to make objects serializable, which all runs on top of the Object Request Broker (ORB).</p>
<h3 id="types-of-middleware">Types of Middleware</h3>
<h4 id="reflection">Reflection</h4>
<p>This method can examine the properties of an object and possibly modify it.</p>
<h4 id="adaption">Adaption</h4>
<p>Adaption can change the behavior of the system, for example at compile-time with macros or at load-time with library interpositioning or at run-time with self modifying code.</p>
]]></content>
        </item>
        
        <item>
            <title>Programming Models and System Architecture</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/systemsarchtecture/</link>
            <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/systemsarchtecture/</guid>
            <description>Design Elements of a Distributed System  The Architecture, is it client-server, hierarchical, peer-to-peer, etc. The programmign model that is used, is it remote procedure call (RPC), libary based, actor model based (akka), mapreduce, etc.  Tradeoffs between the different programming modles and the sotware architecture have to be made, based on the function/non-functional requirements, the cost of convenience, which is easier to use based on the hardware for example, and which is needed for a large scale system.</description>
            <content type="html"><![CDATA[<h2 id="design-elements-of-a-distributed-system">Design Elements of a Distributed System</h2>
<ul>
<li>The Architecture, is it client-server, hierarchical, peer-to-peer, etc.</li>
<li>The programmign model that is used, is it remote procedure call (RPC), libary based, actor model based (akka), mapreduce, etc.</li>
</ul>
<p>Tradeoffs between the different programming modles and the sotware architecture have to be made, based on the function/non-functional requirements, the cost of convenience, which is easier to use based on the hardware for example, and which is needed for a large scale system.</p>
<h3 id="programming-model">Programming Model</h3>
<p>The programming model is the abstaraction of the underlying hardware that forms the execution environment.</p>
<p>These models are often based on multiple abstractions such as the model of computation, communication, or data.</p>
<p>Certain programming models exist for parallel and distributed systems, such as pthread. Different models then offer different funcitons, one might be synchronous while another is asynchronous.</p>
<p>MPI (message passing interface) is anothyer programming model that offers message passing on point-to-point links or broadcasting.</p>
<h4 id="middleware">Middleware</h4>
<p>Middleware is the part that goes between the OS (or lower level parts) and the applications, these define a set of protocols and APIs between the applications and the network or OS for services and communtication. Java RMI is an example of this. Three lefes of middleware exist:</p>
<ol>
<li>Basic messaging</li>
<li>Programming primitives (such as RMI)</li>
<li>Full services (platform)</li>
</ol>
<h4 id="tuple-spaces">Tuple Spaces</h4>
<p>Tuple spaces is another form of a programming model which offers a form of distributed shared memory that is paired with synchronization mechanisms.</p>
<h4 id="actors">Actors</h4>
<p>Actors is another well know model where only an actor can access their internal state and messages can be sent (asynchronously) to other actors. Akka is the Scala example of such a model.</p>
<h3 id="system-architecture">System Architecture</h3>
<p>Pthread implies an implicit system model of MIMD (Multiple Instruction Multiple Data), which is shared memory such that threads can use the same memory.</p>
<p>In large scale systems failures are the norm and will happen.
Message passing with RPC is very tough at a large scale (unless there is a minimal known amount of message passing that only happens between certain edges).</p>
<p>The goal is to restict the progamming modes so that the sytem can do more automatically (scheduling, data distribution, fault-tolerance, etc.).</p>
<h4 id="different-system-architectures">Different System Architectures</h4>
<p>Many different architectures exist:</p>
<ul>
<li>Star (centralized): such as clients only communicating with servers but not with other clients. Or Spark nodes only communicating with the master (though sometimes nodes can communicate with each other)</li>
<li>Hierarchical: DNS is the best example for this, and provides a quicker way to send messages. Imagine peer-to-peer for looking up an IP you would have to possible ask everyone, but with DNS you can find it thorugh three traversal from the root down.</li>
<li>Super-peers: peers are connected to other peers that have some additional responsiblity (hence super), and these super nodes are then connected to other super nodes. This replaces the need for servers, as it was used by Skype earlier.</li>
<li>All-peers (decntralized): BitTorrent or other peer-to-peer networks.</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Resource Management and Scheduling</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/management/</link>
            <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/management/</guid>
            <description>Reading Material Before Lecture Google Borg Google Borg is a management system for clusters that run several hundreds of thousands of jobs from many different applications, on many different clusters with each having several thousands machines.
Resource Management in Cloud Clouds contain large quantities of shared resources that provide provide these to large pools of data and compting with a variety of different interfaces. An important part of cloud computing is resource management, as there only are finite resources available.</description>
            <content type="html"><![CDATA[<h2 id="reading-material-before-lecture">Reading Material Before Lecture</h2>
<h3 id="google-borg">Google Borg</h3>
<p>Google Borg is a management system for clusters that run several hundreds of thousands of jobs from many different
applications, on many different clusters with each having several thousands machines.</p>
<h3 id="resource-management-in-cloud">Resource Management in Cloud</h3>
<p>Clouds contain large quantities of shared resources that provide provide these to large pools of data and compting with
a variety of different interfaces. An important part of cloud computing is resource management, as there only are finite
resources available.</p>
<h3 id="slurm">Slurm</h3>
<p>Slurm is a cluster management and scheduling system for Linux clusters that are fault-tolerant and highly scalable, and
it is open source.</p>
<h3 id="energy-efficient-resource-management">Energy Efficient Resource Management</h3>
<p>In order to fulfill the needs of demand on computational resources, cloud providers have to implement energy efficient
hardware and find the tradeoff between performance and energy efficiency, in order to not use enormous amounts of energy,
while still meeting the quality of service (QoS) requirements.</p>
<h2 id="lecture-notes">Lecture Notes</h2>
<h3 id="scheduling">Scheduling</h3>
<p>The goal of scheduling is to achieve some form of mutual agreement between the resource providers (operators) and
consumers (e.g clients). It also aims to maximize the resource utilization and hence will schedule workloads in a
certain way.</p>
<p>The scheduling <strong>mechanism</strong> explains what to do, (ie.e use this policy for making a decision), and the scheduling
<strong>policy</strong> explains how to do something (schedule with FIFO or randomly).</p>
<p>There are many different scheduling policies such as FIFO (does not always achieve minimal avg RT: average return time
that is the addition of the return times of all tasks divided by the number of tasks), or shortest job first (suffers
from starvation if a continuous stream of short jobs get scheduled when a long job is waiting to be run), or more
advanced policies such as time slicing, which is used in OS, gives jobs a certain time slice and interrupts them once
the time is done. Time slices can be assigned in round robin fashion. There are other techniques such as proportional
share (jobs get resources proportional to what they require). Time slicing is efficient in OS but not for cluster
scheduling.</p>
<p>The scheduler has the jobs to manage the environment, goals and other metrics for optimization, and based on the information
schedule incoming workloads efficiently. Typically schedulers consumer 5% of the cycles in datacenters (relatively low
overhead considering the overhead of more advanced optimizations).</p>
<h3 id="portfolio-scheduling">Portfolio Scheduling</h3>
<p>Since datacenters cannot work without a scheduler, here is proposed to use a set of schedulers, and apply some scheduler
for some period, analyze its performance, and then use the next scheduler and repeat this.</p>
<h3 id="types-of-jobs-in-ds">Types of Jobs in DS</h3>
<ul>
<li>Parallel jobs: high-performance computing</li>
<li>Bags-of-tasks: bag tasks together (e.g. parameter sweep computing (PSC)), high throughput computing</li>
<li>Workflows: MapReduce, event streaming and applying filters</li>
</ul>
<h3 id="architecture-for-distributed-management">Architecture for Distributed Management</h3>
<p>The goal is to execute the full workloads while ensuring the service level agreement (SLA) and objectives (SLO) are met.
The issues come from how resources are allocated, the complexity of the software that manages the architecture, load
imbalance of different machines being used much more than others, having central points (single point of failure), and
having the ability to scale significantly.</p>
<p>Single Cluster Architecture: Uses a single headnode and several nodes to which the headnode then assigns jobs (master
and worker nodes).</p>
]]></content>
        </item>
        
        <item>
            <title>Functional Requirements</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/functionalrequirements/</link>
            <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/functionalrequirements/</guid>
            <description>Reading Material Before Lecture Remote Procedure Call (RPC) RPC allows one program to call a procedure (subroutine or method) in another address space than its own, typically on another system, without the programmer explicitly implementing this remote interaction but instead it is the same as if a local process would invoke some method, except now it comes from a non-local process.
Message Bus Since distributed systems rely on message passing between different systems for sharing of data, the message bus enables this with the addition of hotplugging systems to this at any point in time without affecting any other system.</description>
            <content type="html"><![CDATA[<h2 id="reading-material-before-lecture">Reading Material Before Lecture</h2>
<h3 id="remote-procedure-call-rpc">Remote Procedure Call (RPC)</h3>
<p>RPC allows one program to call a procedure (subroutine or method) in another address space than its own, typically on
another system, without the programmer explicitly implementing this remote interaction but instead it is the same as
if a local process would invoke some method, except now it comes from a non-local process.</p>
<h3 id="message-bus">Message Bus</h3>
<p>Since distributed systems rely on message passing between different systems for sharing of data, the message bus enables
this with the addition of hotplugging systems to this at any point in time without affecting any other system.</p>
<h3 id="lamport-timestamps">Lamport Timestamps</h3>
<p>In order to provide ordering on messages in distributed systems, since different nodes are not synchronized, Lamport
timestamps can be used to timestamp events with a simple and lightweight algorithm.</p>
<h3 id="vector-clocks">Vector Clocks</h3>
<p>Vector clocks are a more advanced algorithm for generating causal ordering (adhering to the HB relation) in which each
message contains the state of the logical clock of the sender. This is an array the size of the number of processes (an
index for each process) of clocks for the state of each process.</p>
<p>The difference between Lamport and Vector clocks is that Lamport is only a single value (single clock) whereas Vector
is a collection of clocks of all processes in the system.</p>
<h3 id="flat-naming">Flat Naming</h3>
<p>It is vital for distributed systems to have some efficient naming scheme for the different systems/nodes, which is used
as their identifier or to determine their location and many more. With flat naming, the identifier is just a set of bits.</p>
<h3 id="hierarchical-naming">Hierarchical Naming</h3>
<p>With hierarchical naming the system or network is divided into subparts of one domain with a single top-level (root)
domain and ever expanding sub-domains until there are some leaf nodes.</p>
<h3 id="domain-name-system-dns">Domain Name System (DNS)</h3>
<p>This is a hierarchical naming system for decentralized systems. It effectively implements efficient naming and allows
convenient partitioning, which in turn allows desired allocation, i.e. many nodes of the leaf domain and only a few
top-level domain nodes.</p>
<h3 id="lightweight-directory-access-protocol-ldap">Lightweight Directory Access Protocol (LDAP)</h3>
<p>LDAP is a directory service similar to flat naming in order to provide lightweight directory naming by associating a
record in a directory entry with an associated attribute such that each record is represented as a (attribute, value)
pair.</p>
<h2 id="lecture-notes">Lecture Notes</h2>
<h3 id="raft">Raft</h3>
<p><strong>Safety property:</strong> when there are no byzantine failures there should never be an incorrect result.<br>
<strong>Availability:</strong>  $n/2+1$ servers required to produce valid result.<br>
<strong>No clocks:</strong> Nodes do not depend on clocks.<br>
<strong>Stragglers:</strong> It can handle stragglers (nodes making process very slowly) if $N/2+1$ servers vote in the end.</p>
<p>Leaders are elected for each term and each leader keeps track of the current term number, if election failed the term
has no leader.</p>
<h3 id="replication">Replication</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Reliability/Redundancy</li>
<li>geographical replication and more nodes to handle client requests</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Lower performance due to the need to maintain consistency across all servers (CAP theorem, atomic operations difficult
on many servers)</li>
<li>Consistency also uses up resources (to manage consistency, i.e. SMR protocols)</li>
</ul>
<p>Amdahl&rsquo;s Law that is given as</p>
<p>$$S(n)=\frac{1}{(1-P)+\frac{P}{n}}$$</p>
<p>stating that speedup of an application is a function of how parallelizable the application is, throwing unlimited resources
at it will at some point not lead to any performance gains.</p>
<p>Replication systems have <em>Leaders</em> or <em>Masters</em>, which are the servers that take the client requests, and <em>Followers</em>, <em>slaves</em>,
or <em>Replicas</em> that provide the read only data access. Systems can have a single leader (master-slave configuration), multiple
leaders (master-master configuration) opr be leaderless replication where all nodes are the same.</p>
<p>When the system is synchronous the writes need to be configured by a specified number of slaves before it can be considered
successful. In an asynchronous system the master can classify it as successful immediately and the slaves apply changes
when they are ready.</p>
<p><strong>Push-based</strong> systems are where servers provide the modifications to the slaves, whereas <strong>pull-based</strong> is where clients
are asking for updates. <strong>Leases</strong> allow for some time the server will send updates but once the lease expires the
client will have to start pulling or renew the lease to get the updates from the master. Leases can be age-based, renewal-
frequency based (clients that request more often get longer leases), or overhead based (busy servers give shorter leases).</p>
<h3 id="consistency">Consistency</h3>
<p><strong>A</strong>tomicity: Updates are either done full or not at all.<br>
<strong>C</strong>onsistency: The system will always be in a valid state, transitions are only from valid to valid.<br>
<strong>I</strong>solation: Transactions do not interfere with each other.<br>
<strong>D</strong>urability: Modifications of successfully commited transactions are never lost.</p>
<p>Two schedules are <strong>Equivalence schedules</strong> if they work on the same transactions and the conflict pairs are ordered in
the same way. A schedule is then considered to be correct if it is equivalent to a serializable schedule and hence is
also serializable. Serializabily differs from linearizability in that it works on multiple objects in arbitrary order
while linearizability works on a single object in real time order.</p>
<p>This can be achieved by using <strong>Two-Phase Locking</strong> which means that in order to participate in a transaction it needs
to acquire all necessary locks for that object (rule 1), and once a transaction releases a lock it cannot acquire any
more locks (rule 2).</p>
<p>In a distributed system this can be achieved with <strong>Two-Phase Commit (2PC)</strong> where the coordinator sends a <em>VOTE_REQ</em> to
all slaves, and they respond accordingly <em>VOTE_COMMIT</em> or <em>VOTE_ABORT</em> depending on if the slave succeeded with the transaction,
and if the coordinator receives a single abort he sends a <em>GLOBAL_ABORT</em> to all slaves to abort everything, otherwise if
he receives no abort he sends a <em>GLOBAL_COMMIT</em> to all the slaves.</p>
]]></content>
        </item>
        
        <item>
            <title>Introduction</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/introduction/</link>
            <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/introduction/</guid>
            <description>What is the difference between parallel computing and distributed computing?
In parallel computing there typically is one job that is split into multiple tasks, which are then split up to different cores/threads that run simultaneously. Such systems typically are homogenous, the hardware is all the same, i.e. the threads run in the same system on the same CPU.
In distributed computing there are also multiple tasks, but these can come from one job or possibly multiple jobs.</description>
            <content type="html"><![CDATA[<p>What is the difference between parallel computing and distributed computing?</p>
<p>In parallel computing there typically is one job that is split into multiple tasks, which are then split up to different
cores/threads that run simultaneously. Such systems typically are homogenous, the hardware is all the same, i.e. the
threads run in the same system on the same CPU.</p>
<p>In distributed computing there are also multiple tasks, but these can come from one job or possibly multiple jobs. These
tasks run on heterogenous hardware, different computers in different locations with different specifications, and hence
requires some synchronization to keep the overall system in a legal state.</p>
<p>Problems arise due to the need for synchronization, which is achieved by message exchanging, which is far from perfect.
Messages can be lost, corrupted, or delayed, thus the communication is asynchronous.</p>
<p>Distributed systems can run in a shared state, shared memory, or not in a shared state, where there is no notion of a
common clock and all communication happens with messages. Distributed systems then have the following properties</p>
<ul>
<li><strong>C</strong>onsistency: stating that there exists one copy of the data that is up-to-date</li>
<li><strong>A</strong>vailability: stating that it always has to be possible to modify or retrieve some data</li>
<li><strong>P</strong>artition Resilience: stating that if the network were to be interrupted, the system can handle this</li>
</ul>
<p>An example of a distributed system is a distributed hash table (DHT), where individual nodes have finger tables such that the number of nodes to send a message to for finding the successor is in the order of $O(\text{log }n)$. If nodes fail, others can take over since data is replicated over different nodes. But this fails with network partitions as it would create two different rings.</p>
<p>Hence when designing distributed systems it is about finding the tradeoff of the three properties for the requirements of the system.</p>
<p>Another distributed system is a data cache that keeps time stamped information and a secondary engine then runs, which is permanently active, to retrieve the information from the system and update the cache.</p>
]]></content>
        </item>
        
        <item>
            <title>Info</title>
            <link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/info/</link>
            <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
            
            <guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/info/</guid>
            <description>IN4391 Distributed Systems notes at TUDelft (Q3).
Studyguide: https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55274 Book: Andrew S. Tanenbaum, Maarten Van Steen, Distributed Systems, Principles and Paradigms (2nd Edition), Prentice Hall, 2006.
The content is based on the lecture material and summarized by myself, due to licensing no actual content of the course is copied here and any images and phrases used come from the book.</description>
            <content type="html"><![CDATA[<p>IN4391 Distributed Systems notes at TUDelft (Q3).</p>
<p>Studyguide: <a href="https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55274">https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55274</a>
Book: Andrew S. Tanenbaum, Maarten Van Steen, Distributed Systems, Principles and Paradigms (2nd Edition), Prentice Hall, 2006.</p>
<p>The content is based on the lecture material and summarized by myself, due to licensing no actual content of the course is copied here and any images and phrases used come from the book.</p>
]]></content>
        </item>
        
    </channel>
</rss>
