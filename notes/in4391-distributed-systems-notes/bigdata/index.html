<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=author content>
<meta name=description content="IN4150 Distributed Systems Notes">
<meta name=keywords content>
<meta name=robots content="noodp">
<meta name=theme-color content>
<link rel=canonical href=https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/bigdata/>
<title>
Big Data and Distributed ML
</title>
<link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css>
<link rel=stylesheet href=/main.89fa80e2143f71bd5c96a7c94e531dec3276a367e22f87e74a76026ab64680bf.css>
<meta itemprop=name content="Big Data and Distributed ML">
<meta itemprop=description content="IN4150 Distributed Systems Notes"><meta itemprop=datePublished content="2021-03-26T00:00:00+00:00">
<meta itemprop=dateModified content="2021-03-26T00:00:00+00:00">
<meta itemprop=wordCount content="689"><meta itemprop=image content="https://nicktehrany.github.io">
<meta itemprop=keywords content>
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://nicktehrany.github.io">
<meta name=twitter:title content="Big Data and Distributed ML">
<meta name=twitter:description content="IN4150 Distributed Systems Notes">
<meta property="og:title" content="Big Data and Distributed ML">
<meta property="og:description" content="IN4150 Distributed Systems Notes">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/bigdata/"><meta property="og:image" content="https://nicktehrany.github.io"><meta property="article:section" content="notes">
<meta property="article:published_time" content="2021-03-26T00:00:00+00:00">
<meta property="article:modified_time" content="2021-03-26T00:00:00+00:00"><meta property="og:site_name" content="Nick Tehrany">
<meta property="article:published_time" content="2021-03-26 00:00:00 +0000 UTC">
</head>
<body>
<div class=container>
<header class=header>
<span class=header__inner>
<a href=/ style=text-decoration:none>
<div class=logo>
<span class=logo__mark>$</span>
<span class=logo__text>cd nicktehrany</span>
<span class=logo__cursor>
</span>
</div>
</a>
<span class=header__right>
<nav class=menu>
<ul class=menu__inner><li><a href=/awards>Awards</a></li><li><a href=/publications>Publications</a></li><li><a href=/projects>Projects</a></li><li><a href=/posts>Blog</a></li><li><a href=/notes>Uni Notes</a></li>
</ul>
</nav>
<span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg>
</span>
</span>
</span>
</header>
<div class=content>
<main class=posts>
<div class=post-info>
<p>
</p>
</div>
<article>
<h1 class=post-title>
<a href=https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/bigdata/>Big Data and Distributed ML</a>
</h1>
<div class=post-content>
<p>Three different models of parallel computing:</p>
<ol>
<li>Shared memory with multiple CPUs connected to a cache-coherent interconnect to multiple RAM modules, to which all have access.</li>
<li>Distributed memory with multiple CPUs all being connected to their own RAM module and a non-cache-coherent interconnect.</li>
<li>OpenMP where data is used where the computation is, which is the interface for multi-plaotform shared memory multiprocessing.</li>
</ol>
<h3 id=big-data>Big Data</h3>
<h4 id=first-generation-distributed-file-systems>First Generation Distributed File Systems</h4>
<p>The first open-source system is HDFS, which is a block based file system with built in replication and metadata journaling. It typically keeps 3 data nodes that receive the writes and the name node receives the addBlock that the client has added a block of data, after which then the data nodes confirm to the name node that they have received the new data.</p>
<p>MapReduce on the other hand works with map workers that apply some mapping function on the data, and the reduce worker that reduce the data of the mappers (e.g. summing counts of words). It first splits the input into chunks, which then go to the mapper. A possible optimization is to pre-reduce data during the map phase to save network bandwidth (this can be done if the function is commutative and associative such as adding to each element in an array individually). Lastly the data of the mappers is shuffled, since the reduce node depends on the data of all the mappers.</p>
<h4 id=second-generation-systems>Second Generation Systems</h4>
<p>Apache Spark is the next gen system that uses Resilient Distributed Datasets (RDD) of which partitions can be kept in memory, and it works on course granular transformations instead of fine grained ones. For this Spark has a master node that splits up the tasks and multiple executor nodes of which one is the driver. Executor nodes are then also used for replication of partitions of the RDDs. RDDs are partitions that are immutable (read-only). However also here the reducer has a wide depency (it depends on all the data of all the mappers). To solve this we can shuffle using a hash function that then maps the result of each map operation on a partition to the same result partition for the reducer. The problem with this is that there are a lot of files for each mapper writing its results to the same partition. To solve this a consolidate hash function is used that creates only a single file for a partition to which the mappers write their result, but this can generate random I/O instead of sequential I/O, which is bad for performance.</p>
<p>To solve this a sortShuffle is used in which partitions are append only maps and as soon as they fill up (in memory) data is spilled to a file. These files are sorted and indexed such that the reducer can then use some sorting algorithm (e.g. minHeapMerge) to reduce based on already sorted values. This way there is only a narrow dependency from the resulting partitions to the data partitions from the mappers. The group by key would still have a wide dependency, therefore Spark uses execution stages which end at shuffle boundaries and allow for full synchronization between stages. Stages themselves will be parallelized.</p>
<h3 id=distributed-ml>Distributed ML</h3>
<p>There are two kinds of parallelism in distributed ML, data parallel in which data is split and trained by the same model in parallel, and model parallel where different models are used on the same data to then combine the model in the end.</p>
<p>Distributed ML can be done centralized with different ML models combining results in some ensembling node which then gives the final trained model, or in a tree structure where data goes to every node, each node sends its gradient to the root of the tree (or its forwarded there across other nodes) and the root combines it and sends it down the tree.</p>
<p>The same can be done with a parameter server (just a kv-store) to which all nodes send their results and at which point the final trained model is computed. Or this can be done fully distributed in a P2P network where nodes inform each other about the training and train based on this.</p>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</div>
</article>
<hr>
<div class=post-info>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
689 Words
</p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
2021-03-26
</p>
</div>
<div class=pagination>
<div class=pagination__title>
<span class=pagination__title-h>Read other posts</span>
<hr>
</div>
<div class=pagination__buttons>
<span class="button next">
<a href=https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/middleware/>
<span class=button__text>Middleware</span>
<span class=button__icon>â†’</span>
</a>
</span>
</div>
</div>
</main>
</div>
<footer class=footer>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css>
<div>
<div>
&nbsp; <a href=https://github.com/nicktehrany target=_blank rel=noopener title=Github><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a> &nbsp;&nbsp; <a href=https://www.linkedin.com/in/nicktehrany/ target=_blank rel=noopener title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a> &nbsp;
<a href=/misc/cv.pdf target=_self><i class="ai ai-cv ai-2x"></i></a>
<a href=https://www.semanticscholar.org/author/Tehrany/2105151990 target=_self><i class="ai ai-semantic-scholar ai-2x"></i></a>
</div>
</div>
<div class=footer__inner>
<div class=footer__content>
<span>Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>Themed by <a href=https://github.com/rhazdon/hugo-theme-hello-friend-ng>Hello Friend NG</a></span>
</div>
</div>
</footer>
</div>
<script type=text/javascript src=/bundle.min.2ce64ea6ea44a72b13dd812fc2eb5cca3efe878cce258a47c137c17edf46e0602a05e422b618a5b80b5939c731b7a293351c2f2222a21f6ee27e54a8448dd20e.js integrity="sha512-LOZOpupEpysT3YEvwutcyj7+h4zOJYpHwTfBft9G4GAqBeQithiluAtZOccxt6KTNRwvIiKiH27iflSoRI3SDg=="></script>
</body>
</html>