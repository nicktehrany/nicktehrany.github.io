<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>IN4331 Web-Scale Data Management on Nick Tehrany</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/</link><description>Recent content in IN4331 Web-Scale Data Management on Nick Tehrany</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 15 Apr 2021 08:34:56 -0600</lastBuildDate><atom:link href="https://nicktehrany.github.io/notes/in4331-web-scale-data-management/index.xml" rel="self" type="application/rss+xml"/><item><title>Deterministic DBs</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/deterministic-dbs/</link><pubDate>Mon, 21 Jun 2021 13:25:36 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/deterministic-dbs/</guid><description>Calvin is a transactional scheduling and data replication layer that runs alongside non-transactional storage systems. It uses a deterministic locking protocol (DPL) which eliminates the need of distributed commit protocols (e.g. 2PC), it also uses a shared nothing architecture. The model it uses is a deterministic database, which works by knowing the exact read and write sets (which keys are going to be accessed) before starting the transaction. Then if we know what replicas a transaction will touch, the individual operations can be sent to the replica individually and be serialized that way.</description><content type="html">&lt;p>Calvin is a transactional scheduling and data replication layer that runs alongside non-transactional storage systems.
It uses a deterministic locking protocol (DPL) which eliminates the need of distributed commit protocols (e.g. 2PC), it
also uses a shared nothing architecture. The model it uses is a deterministic database, which works by knowing the exact
read and write sets (which keys are going to be accessed) before starting the transaction. Then if we know what replicas
a transaction will touch, the individual operations can be sent to the replica individually and be serialized that way.
Therefore, clients will contact a sequencer which will create the order of operations for the transactions. If there are
multiple sequencers, the sequencers will individually use Paxos to decide on the order (if clients send requests to two
different sequencers for example), and sequencers in Calvin will talk to the scheduler every 10ms to schedule a batch of
operations. The order of operations is created by the sequencer, it will pass this to the scheduler, which then applies
the schedule for the actual locking.&lt;/p></content></item><item><title>Google Spanner: Google's Globally Distributed Database</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/googlespanner/</link><pubDate>Wed, 09 Jun 2021 14:52:13 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/googlespanner/</guid><description>Spanner provides a unique feature of providing distributed transactions without locks, through the property of Spanner that distributed transactions are externally consistent. Instead of locking the database for reads to get the data (all writes are locked), it works by generating a snapshot of the database from which the reads happen (writes can then still happen on the original database), however now snapshots need to be synchronized across the distributed shards of the database.</description><content type="html"><![CDATA[<p>Spanner provides a unique feature of providing distributed transactions without locks, through the property of Spanner
that distributed transactions are externally consistent. Instead of locking the database for reads to get the data (all
writes are locked), it works by generating a snapshot of the database from which the reads happen (writes can then still
happen on the original database), however now snapshots need to be synchronized across the distributed shards of the
database. Additionally, it uses transactions with strict 2PL (2 phase locking), where each transaction $T$ gets a
timestamp $s$ and data written by transaction $T$ is then timestamped with $s$.</p>
<h3 id="synchronized-snapshots">Synchronized Snapshots</h3>
<p>In order to provide synchronized snapshots, Spanner needs to have some notion of a global clock, which it achieves
through external consistency. This is the property of the commit order respecting the global time order. For achieving
this, timestamp order is used as the commit order (timestamp_order == commit_order). For generating a timestamp, it
uses TrueTime.</p>
<h3 id="truetime">TrueTime</h3>
<p>Presents a global time with a bounded uncertainty given <code>TT.now()</code> is given in a window with an <code>earliest</code> and
<code>lastest</code> time which falls in the interval of $2*\varepsilon$. Then picking a time <code>TT.now().latest</code> which is an upper
bound on time in the future gives that timestamp $s$ falls within the interval of the releasing of locks. This is done
by waiting out the uncertainty in clocks (waiting for actual releasing of clocks), called the *commit wait*, which in
turn is done by waiting until <code>TT.now().earliest</code> &gt; $s$. Therefore the average commit wait is roughly $2*\varepsilon$.</p>
]]></content></item><item><title>Paxos Megastore</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/paxosmegastore/</link><pubDate>Tue, 25 May 2021 17:05:50 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/paxosmegastore/</guid><description>Paxos is a distributed systems consensus algorithm that can tolerate non-malicious failures. It provides safety that only a value that has been proposed by some node will be the chosen one, only a single value is chosen, and only chosen values are communicated further to all nodes. Liveness is provided by guaranteeing that some proposed value will eventually be chosen, and that if a value has been chosen, every node will eventually know about this choice.</description><content type="html"><![CDATA[<p>Paxos is a distributed systems consensus algorithm that can tolerate non-malicious failures. It provides safety that
only a value that has been proposed by some node will be the chosen one, only a single value is chosen, and only chosen
values are communicated further to all nodes. Liveness is provided by guaranteeing that some proposed value will
eventually be chosen, and that if a value has been chosen, every node will eventually know about this choice.</p>
<h3 id="phase-1">Phase 1</h3>
<p>The <em>prepare</em> phase in which a proposer (which is acting as the leader) creates a proposal, with number <em>n</em>, and sends a
prepare request to all acceptors. The acceptors receive the request and if <em>n</em> is the largest proposal they know of,
they accept and will not accept any proposal with a lower value by sending a <em>promise</em>.</p>
<h3 id="phase-2">Phase 2</h3>
<p>The <em>accept</em> phase proceeds with if the proposer has enough promises from acceptors (from the quorum), it sets a value
for its proposal. It then sends an <em>accept</em> message to all acceptors. The acceptors then accept the proposal iff it has
promised to accept this proposal. Acceptors can then send the learned value to all the learners (e.g. the clients).</p>
]]></content></item><item><title>Streams</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/streams/</link><pubDate>Mon, 24 May 2021 18:22:43 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/streams/</guid><description>Data can be static (it does not change, is written once and never updated), therefore most commonly used is streaming data, which changes (changes at some point in time, does not have to be frequently or soon but it will eventually change) and is updated. A data stream is a stream of events with each element having a timestamp.
Types of data:
Bounded data: a dataset with a beginning and an end.</description><content type="html"><![CDATA[<p>Data can be static (it does not change, is written once and never updated), therefore most commonly used is streaming
data, which changes (changes at some point in time, does not have to be frequently or soon but it will eventually
change) and is updated. A data stream is a stream of events with each element having a timestamp.</p>
<p>Types of data:</p>
<ul>
<li><strong>Bounded data:</strong> a dataset with a beginning and an end.</li>
<li><strong>Unbounded data:</strong> a dataset without a beginning or an end.</li>
</ul>
<p>Types of processing:</p>
<ul>
<li><strong>Stream processing:</strong> continuous stream of processing that continuously produces new results, systems such as Apache
Flink or Apache Storm</li>
<li><strong>Batch processing:</strong> processing which takes a finite amount of time to complete and for it to produce results,
systems such as Apache Hadoop MapReduce and Apache Spark</li>
</ul>
<h3 id="stream-processing">Stream Processing</h3>
<p><strong>Streaming window</strong> is a subset of a stream, such as the average value for a stream within a certain time frame, e.g.
10-12pm and another average for 9-11pm, which is then a <em>sliding window</em> since they overlap. Another type of stream
windows is a <em>fixed/tumbling</em> window which is consecutive windows, e.g. from 9-10pm and then 10-11pm. Lastly, there are
<em>jumping windows</em> which just skip parts of the stream, e.g. from 4-5pm and then 9-10pm.</p>
<p>Session windows for users are then seen as a period of activity for a user followed by a period of inactivity, on which
processing can then be done.</p>
<h3 id="definitions">Definitions</h3>
<ul>
<li><strong>Processing time:</strong> the time measured by the machine that executes the operator</li>
<li><strong>Event time:</strong> the timestamp of an event given in its data record</li>
<li><strong>Ingress/Ingestion time:</strong> the timestap of the first operator in the program that sees this event</li>
</ul>
]]></content></item><item><title>Bigtable</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/bigtable/</link><pubDate>Wed, 19 May 2021 16:03:09 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/bigtable/</guid><description>Bigtable is a high-performance proprietary database which is used by multiple google services (or was used, has several newer versions mostly replacing it). It is meant for large throughput with very large files. Requirements are that processes update continuously and asynchronously. Additionally, it is required to be fault tolerant, persistent, and can use and work efficiently on cheap hardware, and scale well.
Google File System (GFS) A file system for very large files.</description><content type="html"><![CDATA[<p>Bigtable is a high-performance proprietary database which is used by multiple google services (or was used, has several
newer versions mostly replacing it). It is meant for large throughput with very large files. Requirements are that
processes update continuously and asynchronously. Additionally, it is required to be fault tolerant, persistent, and can
use and work efficiently on cheap hardware, and scale well.</p>
<h3 id="google-file-system-gfs">Google File System (GFS)</h3>
<p>A file system for very large files. Typical systems have block size of 4KiB (PageSize) and GFS has larger blocks of
minimally 64MB such that storing smaller files below 64MB is a waste of space and very large files can be stored much
more efficiently. GFS works with a master node and several replica nodes (to store each file 3 times). The master node
is there to maintain information on where blocks are located, and managing defragmentation and other file system jobs.
Additionally, GFS is an append-only file system, which is beneficial for google since files are rarely modified.</p>
<h3 id="bloom-filters">Bloom Filters</h3>
<p>Instead of using indexing which has a high overhead, Bigtable uses bloom filters to identify if elements <em>might</em> be in
the certain SSTable. Bloom filters use multiple hash functions which hash the key you are looking for, and then sets
the bits in a bit array to 1 according to the result of the hash function. When looking for a key, it again is hashed
and the according bits are checked, if any of the bits is 0 we know that the element is not in the set, however if all
bits checked by the hash are 1 the value <em>might</em> be in the set (as combination of other keys might have set those same
bits to 1 the key might not be there).</p>
]]></content></item><item><title>Parallel Analytical Processing</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/parallel_analytical_processing/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/parallel_analytical_processing/</guid><description>Relational DBs vs. Modern Big Data Processing Stack Relational dbs such as SQL go through a stack that starts with the SQL compiler &amp;amp; optimizer and then executes them on a relational dataflow engine, on top of the transaction manager, which then is on top of the storage and the OS. Big data on the other hand, also has the SQL compiler &amp;amp; optimizer which then runs of top of different dataflow engines (Flink, Spark, Hadoop).</description><content type="html"><![CDATA[<h3 id="relational-dbs-vs-modern-big-data-processing-stack">Relational DBs vs. Modern Big Data Processing Stack</h3>
<p>Relational dbs such as SQL go through a stack that starts with the SQL compiler &amp; optimizer and then executes them on a
relational dataflow engine, on top of the transaction manager, which then is on top of the storage and the OS. Big data
on the other hand, also has the SQL compiler &amp; optimizer which then runs of top of different dataflow engines (Flink,
Spark, Hadoop). Essentially running a dataflow engine on top of a distributed file system.</p>
<h3 id="parallelism-in-databases">Parallelism in Databases</h3>
<ul>
<li><strong>Inter-query Parallelism (concurrent queries):</strong> Multiple queries can be run in parallel</li>
<li><strong>Pipeline Parallelism (concurrent operators):</strong> Multiple parts of a query plan can be executed in parallel</li>
<li><strong>Data Parallelism:</strong> Multiple cores/threads can work on the same operation at the same time but on different parts of
the data</li>
</ul>
<p>The maximum degree of data parallelism is the maximum number of possible data partitions.</p>
<h3 id="forms-of-data-partitioning">Forms of Data Partitioning</h3>
<p>Due to <strong>round-robin</strong> partitioning, in which there is no grouping (then can only do filter operation, not join
aggregate or other operations) but it is random and balanced in size of each partition, therefore <strong>hash partitioning</strong>
can be used, which groups data by the hash (which can then be on some data that should be joined or aggregated by).
Another approach is <strong>range partitioning</strong> which splits the data by ranges (e.g. from 0-9 is 1 partition, 10-19 another
etc.).</p>
]]></content></item><item><title>Scalable Data Integration and Discovery in Data Lakes</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/datalake/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/datalake/</guid><description>Data Lakes The problem starts in the management of big data, which has the following requirements:
Volume: the data is typically large Variety: the type of the data varies Velocity: the data needs to be accessed and processed quickly Veracity: the quality of the data needs to be maintained and guaranteed Value: the data is used for intended purposes and holds value to that purpose Data lakes are a way of storing large amounts of data without strict structure and guidelines, and all data is just &amp;ldquo;dumped&amp;rdquo; there.</description><content type="html"><![CDATA[<h2 id="data-lakes">Data Lakes</h2>
<p>The problem starts in the management of big data, which has the following requirements:</p>
<ul>
<li><strong>Volume:</strong> the data is typically large</li>
<li><strong>Variety:</strong> the type of the data varies</li>
<li><strong>Velocity:</strong> the data needs to be accessed and processed quickly</li>
<li><strong>Veracity</strong>: the quality of the data needs to be maintained and guaranteed</li>
<li><strong>Value</strong>: the data is used for intended purposes and holds value to that purpose</li>
</ul>
<p>Data lakes are a way of storing large amounts of data without strict structure and guidelines, and all data is just &ldquo;dumped&rdquo; there. However, this can quickly lead to data swamps, therefore the data lake is more of a set of centralized repositories of raw data, which can be structure or unstructured. There must be metadata to describe and organize the data and be able to quickly query or process the data.</p>
<h2 id="data-integration">Data Integration</h2>
<p>Data integration is a unified and transparent way of providing access to the raw data in a data lake or other data sources. These data sources can be heterogeneous (structure, content, sources, etc.). There are two types of architectures for data integration:</p>
<ol>
<li><strong>Data Warehouse:</strong> a single data store where the data from different sources is stored</li>
<li><strong>Virtual Integration:</strong> queries are applied online after query reformulation on the mediated data schema. This way the data does not need to leave the database.</li>
</ol>
<h2 id="dataset-discovery">Dataset Discovery</h2>
<p>Due to the heterogeneity in data lakes data scientists need to find relevant data from known data sets (find related data to the ones that the data scientist has). <em>Schema Matching</em> is component for finding related data sets.</p>
<p>Tabular datasets can present relatedness which means that tables are joinable if there are overlapping columns which can be joined. Unionable tables store the same information but also have other data points, then extending data sets is just adding more data points on the same key from different datasets.</p>
<p>In order for finding overlaps in metrics the Jaccard similarity can be used which is given as for two domains $X$ and $Y$,</p>
<p>$$s(X,Y)=\frac{\lvert X \cap Y \rvert}{\lvert X \cup Y \rvert}$$</p>
<p>The higher the Jaccard similarity is the more similarities the two domains have, however it is biased for smaller datasets, as there the Jaccard similarity can more easily reach a higher value.
For resolving this issue we can use set containment, which is given as for two domains $X$ and $Y$,</p>
<p>$$t(X,Y)=\frac{\lvert X \cap Y \rvert}{\lvert X \rvert}$$</p>
<p>as this will ignore the size of the set, and only include the actual overlap.</p>
<p>Additionally, we have the set overlap given as,</p>
<p>$$o(X,Y)=\lvert X \cap Y \rvert$$</p>
<p>which is just the intersection of two sets and can be used for finding joinable tables.</p>
<h3 id="dataset-discovery-optimizations">Dataset Discovery Optimizations</h3>
<p>As this is very slow (due to having to do many calculations in data lake), there have been several proposals for accelerating this procedure</p>
<h4 id="lsh-ensemble">LSH Ensemble</h4>
<p>Use a Minwise hash function that will return the minimum hash value of all the hashes of a column, which can then be used to approximate the Jaccard similarity.</p>
<p>Additionally, there is Locality Sensitive Hashing which prunes the search space of the dataset by mapping signatures of domains to different buckets (not hashing all columns individually).</p>
<h4 id="lazo">Lazo</h4>
<p>Using One-Permutation Hashing (OPH) it improves on MinHash LSH by using a single hash function instead of one hash function per domain as in LSH.</p>
<h4 id="josie">JOSIE</h4>
<p>Extracts the top most joinable tables given a query column and finding the data in the data lake with the highest overlap.</p>
<h4 id="silkmoth">SilkMoth</h4>
<p>Makes related set discovery faster by finding set similarity, set containment, and given a reference set $R$ it can find all related sets</p>
]]></content></item><item><title>NoSQL - Design Principles</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/no_sql_design/</link><pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/no_sql_design/</guid><description>Replication Master-Slave Replication All updates are made directly to the master which then propagates the changes to all the slaves.
P2P Replication Replication without a master node, now updates can be on any of the replicas which then propagate the updates to all the replicas.
Distributed Transactions Two Phase Commit (2PC) Transactions are submitted to a coordinator which then sends a prepare to all workers. The workers log the request and respond to the coordinator.</description><content type="html"><![CDATA[<h2 id="replication">Replication</h2>
<h3 id="master-slave-replication">Master-Slave Replication</h3>
<p>All updates are made directly to the master which then propagates the changes to all the slaves.</p>
<h3 id="p2p-replication">P2P Replication</h3>
<p>Replication without a master node, now updates can be on any of the replicas which then propagate the updates to all the replicas.</p>
<h2 id="distributed-transactions">Distributed Transactions</h2>
<h3 id="two-phase-commit-2pc">Two Phase Commit (2PC)</h3>
<p>Transactions are submitted to a coordinator which then sends a prepare to all workers. The workers log the request and respond to the coordinator. When the coordinator receives a <em>ready</em> from all the workers it sends a commit message to all workers which says that the workers should do all the work that was proposed in the prepare. In the prepare the workers just say if the request is possible. If any of the workers responds with a <em>failure</em> the coordinator will stop the request and tell all the workers to abort via an <em>ABORT</em> message and any preparation work by the worker will be stopped (clean the log).</p>
<h3 id="linearizability-vs-serializability">Linearizability vs. Serializability</h3>
<p>In linearizability there is one correct order of operations on one element that all servers have to agree upon. With serializability there can be multiple orders of operations across servers, however the final state of the database is the same with any of the orders and is consistent to the operations that were run and expected.</p>
<h3 id="base">Base</h3>
<ul>
<li><strong>B</strong>asically <strong>A</strong>vailable</li>
<li><strong>S</strong>oft State</li>
<li><strong>E</strong>ventually Consistent</li>
</ul>
]]></content></item><item><title>P2P Storage</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/p2p/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/p2p/</guid><description>Databases have some sort of model to represent the data in this model,
Relational databases Based on the relational model, it is typically presented in tables that have some relation to each other among which tables can be joined, and include indices.
Application DB The benefit of relational DBs is that they can easily be incorporated into multiple applications since they have defined schemas for the tables and a universal interface.</description><content type="html"><![CDATA[<p>Databases have some sort of model to represent the data in this model,</p>
<h2 id="relational-databases">Relational databases</h2>
<p>Based on the relational model, it is typically presented in tables that have some relation to each other among which tables can be joined, and include indices.</p>
<h3 id="application-db">Application DB</h3>
<p>The benefit of relational DBs is that they can easily be incorporated into multiple applications since they have defined schemas for the tables and a universal interface. Each application has its own data model and applications then communicate via some sort of messaging model. However now since each application has its own data model there is no unique model for all databases (no central data model as there wold be with integration DBs that integrate multiple applications into a single central database).</p>
<p>These types of databases are also easier to manage since there is no one central point where all data needs to go and there is no need for synchronization between the different points accessing the central point (easier to scale, distribute, and load balance).</p>
<h2 id="p2p-storage">P2P Storage</h2>
<p>The goal with P2P systems is to scale out (distribute the data) that also have resilience to failures, such that failing nodes can easily be handled (and replaced).</p>
<p>There are several different P2P systems:</p>
<ul>
<li>
<p><strong>Client-Server (Not P2P!)</strong>: A central server that provides some service or content and clients directly connect to the central server.</p>
</li>
<li>
<p><strong>Centralized P2P</strong>: A central entity to provide a service that is used for indexing of data/groups of where data is located on other peers, and peers are connected to each other. They can request a location of some data from the central server and receive where the data is located and can request the data from that node. This poses a difficulty of load balancing of hot and cold data (what is hot? when is it hot? how often replicate it?), and the system still relied on a central server for indexing.</p>
</li>
<li>
<p><strong>Pure P2P</strong>: All nodes are connected to each other and there is no single entity. This allows for leaving and joining to the system without any loss, however it is difficult to manage indexing and what data is on what node, since each node would need to broadcast the data it has to all other nodes to let them know that they have this data.</p>
</li>
<li>
<p><strong>Hybrid P2P</strong>: This aims to solve the problem of pure P2P by having super peers to which individual peers are connected and super peers are connected to each other. This allows for dynamic central entities that host indexing.</p>
</li>
<li>
<p><strong>Pure P2P (DHT Based)</strong>: Distributed hash tables (DHT) where searching and routing (and having information about other machines) can all be done in $\mathcal{O}(\log{}n)$. Can only be hashed by a single value, so if for example you want to share songs you need multiple DHTs to allow for hashing only by song, by song with artist, and so forth, so that songs can be found without all the information (valid existing hashes can be created without all the info). DHTs have a ring structure (which is where the $\log{}n$ comes from). Solving load balancing can be done by hashing yourself when joining, then have a resulting hash range and have yourself routed along the network to the range where you ended up in (this works since the hash function is uniformly distributed and has the avalanche effect). Finding a hash is done by each node maintaining information of $\log{}n$ other nodes in a <em>finger table</em> where different distances to on the ring are stored and which nodes are responsible for that. The addresses you need are</p>
<p>$$targedId = (currId + 2^i) \text{ mod } 2^m$$</p>
<p>Routing requests for a node can then be directly forwarded to the node by sending it to the most distant known node which is below the value that is being looked up (always cutting the search space in half).</p>
</li>
</ul>
<h3 id="chord">Chord</h3>
<p>Is a generic DHT interface implementation that uses a fixed-size hash space of $2^m-1$ which limits the maximum number
of peers and uses SHA1 for hashing (160 bits which is still a very large number). Nodes maintain a finger table with
nodes that increase exponentially, distances of $2^0,2^1.2^2,&hellip;,2^{m-1}$ which allows to always split the key space in
half on lookups. Finger tables store the distance, the hash ID of the target, and the node responsible for that ID.
(Function for constructing finger table is same as above). Routing on lookup is then done by going to the most distant
node ID in the finger table which is below the value looked for (check TargetID column). Then start routing from that
node again. Repeat until there is no smaller TargetID anymore.</p>
<p>Newly joining nodes just hash themselves, to obtain a new ID then contact necessary DHT nodes via discovery, contact the
new node that is responsible for the new node ID, split the arc responsibility, and construct the finger table.</p>
<h3 id="amazon-dynamo">Amazon Dynamo</h3>
<p>Assumes that there are no malicious nodes (no need for fraud detection), and implements a <code>put()</code> and <code>get()</code> for the storage system based on a DHT. In order to answer queries fast, each node maintains a full finger table (therefore the ring is fully connected) and there are no varying response times in the network. For load balancing each node also creates an additional virtual server so that if a node is under heavy load the virtual server (its partitions it is responsible for) can just be moved to a new node.</p>
<p>Updates (after <code>put()</code>) are propagated asynchronously, resulting in eventual consistency. It also uses read/write quorums such that the <code>put()</code> operation does not return until the majority of replicas have been updated (or however many nodes the quorum requires).</p>
]]></content></item><item><title>Introduction</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/introduction/</link><pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/introduction/</guid><description>Web scale scale data management is the management of large scale datasets with the goal of how to efficiently store, query, and run large amounts of transactions on the datasets.
Multiple different systems exist, such as P2P systems or cloud storage.</description><content type="html">&lt;p>Web scale scale data management is the management of large scale datasets with the goal of how to efficiently store, query, and run large amounts of transactions on the datasets.&lt;/p>
&lt;p>Multiple different systems exist, such as P2P systems or cloud storage.&lt;/p></content></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/info/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/info/</guid><description>IN4331 Web-Scale Data Management notes at TUDelft (Q4).
Studyguide: https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55287
The content is based on the lecture material/slides and summarized by myself.</description><content type="html"><![CDATA[<p>IN4331 Web-Scale Data Management notes at TUDelft (Q4).</p>
<p>Studyguide: <a href="https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55287">https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55287</a></p>
<p>The content is based on the lecture material/slides and summarized by myself.</p>
]]></content></item></channel></rss>