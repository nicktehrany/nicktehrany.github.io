<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>IN4049TU Introduction to High Performance Computing on Nick Tehrany</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/</link><description>Recent content in IN4049TU Introduction to High Performance Computing on Nick Tehrany</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 29 Aug 2021 21:53:20 +0200</lastBuildDate><atom:link href="https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/index.xml" rel="self" type="application/rss+xml"/><item><title>Poisson Equation</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/poisson-equation/</link><pubDate>Fri, 21 Jan 2022 21:23:46 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/poisson-equation/</guid><description>Iterative Methos for Linear Systems Iterations methods are used for solving linear equations (often heat problems, air flow for planes, etc.) with methods such as Jacobi Iteration, Gauss-Seidel iteration and SOR (successive over-relaxation). These iterative methods generate a sequence of approximation vectors that converge to a solution. Evaluations then show how quickly this iteration sequence converges. With Jacobi and Gauss-Seidel, the computation of a new approximation depends on a combination of prior computed approximated vectors, which is why they are called relaxation methods.</description><content type="html"><![CDATA[<h2 id="iterative-methos-for-linear-systems">Iterative Methos for Linear Systems</h2>
<p>Iterations methods are used for solving linear equations (often heat problems, air flow for planes, etc.) with methods such as Jacobi Iteration, Gauss-Seidel iteration and SOR (successive over-relaxation). These iterative methods generate a sequence of approximation vectors that converge to a solution. Evaluations then show how quickly this iteration sequence converges. With Jacobi and Gauss-Seidel, the computation of a new approximation depends on a combination of prior computed approximated vectors, which is why they are called <em>relaxation methods</em>.</p>
<p>Convergence for Jacobi and Gauss-Seidel occurs when the matrix (approximation result) is strongly diagonal dominant, which means the absolute values of the diagonal of the elements in the matrix are significantly larger than that the absolute value of the sums of other rows.</p>
<p>Since often convergence with these two methods is not fast enough, an additional <em>relaxation parameter</em> is added. This is what <strong>Jacobi over-relaxation</strong> is based on. With it the matrix is split with the additional relaxation parameter.</p>
<p><strong>Successive over-relaxation</strong> is Gauss-Seidel with the relaxation parameter. The result from the prior approximation is combined with the elements in the current one to give a new relaxation parameter for each iteration.</p>
<h2 id="parallel-implementation-of-jacobi-iteration-method">Parallel Implementation of Jacobi Iteration Method</h2>
<p>Since each element in the approximation can be calculated individually, we can have a maximum parallelism here of the number of approximation elements. However, since approximations of each element require those of the previous approximation there is a need for some communication.</p>
<h2 id="parallel-implementation-of-gauss-seidel-iteration-method">Parallel Implementation of Gauss-Seidel Iteration Method</h2>
<p>Here all elements in the approximation depend on the prior element in that approximation instead of the prior approximation. This means that this cannot be parallelized, but only the part of actually calculating the element (the scalar for that approximation).</p>
<h2 id="red-black-ordering">Red-black ordering</h2>
<p>Since there is only limited parallelism, there needs to be a reordering that splits the 2D mesh (of grid points that have to exchange approximation elements) into red and black pairs like a checkered board. This way all points can do independent calculations then switch to the other color to do their independent operations (however they then depend on the prior color, which is why always one color is active).</p>
<h2 id="summary">Summary</h2>
<p>For Jacobi SOR and CG it takes $n=N^\frac{1}{2}$ steps with an $nxn$ matrix for the info to have traveled across the matrix.</p>
<h2 id="metrics">Metrics</h2>
<p>Data Locality Ratio is given as</p>
<p>$$\frac{T_{computed}}{T_{communicated}}$$</p>
<p>Communication time is given as, with $L$ message length in bytes, $Latency=Startup$ and $B$ being bandwidth</p>
<p>$$T_{comm}(L)=Startup+\frac{L}{B}$$</p>
<h2 id="pram">PRAM</h2>
<p>Shows a simplified abstraction of parallel hardware, where there are e.g. no communication overheads for memory accesses.
It has four variants:</p>
<ol>
<li><strong>Exclusive-read, Exclusive-write (EREW)</strong>: Each processor can only read and write to its memory not any other belonging to other processors.</li>
<li><strong>Concurrent-read, Exclusive-write (CREW)</strong>: Processors can read from any memory, but can only write to their own.</li>
<li><strong>Exclusive-read, Concurrent-write (ERCW)</strong>: Processors can only read from their own memory but can write to any other belonging to any other processor.</li>
<li><strong>Concurrent-read, Concurrent-write (CRCW)</strong>: Processors can both read and write to any memory.</li>
</ol>
]]></content></item><item><title>Parallel Programming</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming/</link><pubDate>Tue, 14 Sep 2021 15:50:41 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming/</guid><description>For distributed memory MPI is used as programming model, with shared memory OpenMP is used, and for GPUs CUDA is used.
MPI Program With MPI programs, the processes that execute the program all have their own local data. Typically one process will be on one core, each process can then access its own data locally and use message passing to get other data from other cores. In MPI programming the environment needs to be initialized before it can be used, and finalized when it&amp;rsquo;s done.</description><content type="html"><![CDATA[<p>For distributed memory MPI is used as programming model, with shared memory OpenMP is used, and for GPUs CUDA is used.</p>
<h3 id="mpi-program">MPI Program</h3>
<p>With MPI programs, the processes that execute the program all have their own local data. Typically one process will be on one core, each process can then access its own data locally and use message passing to get other data from other cores. In MPI programming the environment needs to be initialized before it can be used, and finalized when it&rsquo;s done. <code>numberOfProcs</code> is set to the total number of processes (e.g. with 4 processes this is 4), and <code>rank</code> is the rank of the process, this starts at 0 (!!), so the first process will have rank 0, second will have rank 2, and so forth.</p>
<h3 id="point-to-point-communication">Point-to-Point Communication</h3>
<p>For enabling communication for processes that don&rsquo;t share memory.</p>
<p><code>MPI_Send</code> and <code>MPI_Recv</code> are both blocking calls, therefore deadlocks can occur and should be avoided.</p>
<p>MPI Datatypes are used to tell it how to interpret binary values from the send buffer/receive buffer, convert them etc. It has a bunch of predefined datatypes such as <code>MPI_CHAR</code> or <code>MPI_INT</code>.</p>
<p>MPI also has <code>MPI_Sendrecv</code> which sends one message and receives another in any order, and buffers (send, and recv buffers) have to different and cannot overlap. There is also the <code>MPISendrecv_replace</code> the first sends the message then receives using the same buffer.</p>
<p><code>MPI_Isend</code> and <code>MPI_Irecv</code> are the non-blocking counterparts to the prior mentioned ones. One could then also use the <code>MPI_Wait</code> to block until it&rsquo;s completed, typically this is done after some work is completed and the process needs the data from the recv.</p>
<p>Four send modes in MPI:</p>
<ul>
<li>Standard mode: Block until the message is fully transferred.</li>
<li>Synchronous mode: Block until a reply (the recv) from the receiver has arrived.</li>
<li>Buffered mode: Block until the message (to be sent) is copied to a buffer (specified by user)</li>
<li>Ready mode: If a receive exists only then does this succeed. Not recommended to be used</li>
</ul>
]]></content></item><item><title>Parallel and Distributed Architecture</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-and-distributed-architecture/</link><pubDate>Tue, 07 Sep 2021 16:56:12 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-and-distributed-architecture/</guid><description>Memory Classifications Based on memory organization:
SM - Shared Memory DM - Distributed Memory Based on access times:
UMA - Uniform Memory Access NUMA - Non-Uniform Memory Access SMP - Symmetric Multi-Processor Based on data and control flows with Flynn&amp;rsquo;s taxonomy:
SISD - Single Instruction Single Data MISD - Multiple Instruction Single Data SIMD - Single Instruction Multiple Data MIMD - Multiple Instruction Multiple Data Extensions of memory systems:</description><content type="html"><![CDATA[<h4 id="memory-classifications">Memory Classifications</h4>
<p>Based on memory organization:</p>
<ul>
<li><strong>SM</strong> - Shared Memory</li>
<li><strong>DM</strong> - Distributed Memory</li>
</ul>
<p>Based on access times:</p>
<ul>
<li><strong>UMA</strong> - Uniform Memory Access</li>
<li><strong>NUMA</strong> - Non-Uniform Memory Access</li>
<li><strong>SMP</strong> - Symmetric Multi-Processor</li>
</ul>
<p>Based on data and control flows with <strong>Flynn&rsquo;s taxonomy</strong>:</p>
<ul>
<li><strong>SISD</strong> - Single Instruction Single Data</li>
<li><strong>MISD</strong> - Multiple Instruction Single Data</li>
<li><strong>SIMD</strong> - Single Instruction Multiple Data</li>
<li><strong>MIMD</strong> - Multiple Instruction Multiple Data</li>
</ul>
<p>Extensions of memory systems:</p>
<ul>
<li>Shared cache, where processors share caches and memory.</li>
<li>Bus-based shared memory (Symmetric Multiprocessors - SMP), where processors share the same memory and bus to it.</li>
</ul>
]]></content></item><item><title>Introduction</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/introduction/</link><pubDate>Tue, 31 Aug 2021 16:27:34 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/introduction/</guid><description>Why do we need powerful computer systems? Because more and more computation is moving to computer systems, experiments, proofs and other theoretical engineering done traditionally on paper. Said engineering is too slow on paper and computationally limited, as well as dangerous sometimes (in scenarios where simulation is needed, e.g. earthquakes, car crashes).
Computational Science Triangle presents the combination of computer systems, applications and algorithms, out of which a model is then constructed.</description><content type="html"><![CDATA[<h4 id="why-do-we-need-powerful-computer-systems">Why do we need powerful computer systems?</h4>
<p>Because more and more computation is moving to computer systems, experiments, proofs and other theoretical engineering
done traditionally on paper. Said engineering is too slow on paper and computationally limited, as well as dangerous
sometimes (in scenarios where simulation is needed, e.g. earthquakes, car crashes).</p>
<p><strong>Computational Science Triangle</strong> presents the combination of computer systems, applications and algorithms, out of
which a model is then constructed. This model is then used to fulfill the computational science paradigm of the
previously mentioned theoretical engineering scenarios.</p>
<p>Three new kinds of HPC have emerged with the need for Big Data:</p>
<ul>
<li><strong>Volume</strong> - large volumes of data being processed</li>
<li><strong>Velocity</strong> - new data is being generated quickly</li>
<li><strong>Variety</strong> - there are a large variety of data formats</li>
</ul>
<p>Additionally, HPC systems require large amounts of memory, as the data is stored in memory for processing.</p>
<p>HPC is also widely used for web applications, such as web crawling, indexing, and sorting (e.g. with MapReduce).</p>
<h4 id="requirements-of-hpc">Requirements of HPC</h4>
<p>Modern HPC relies on:</p>
<ul>
<li>Powerful supercomputers (enough resources to do the computation)</li>
<li>Parallelism in the problem (how far can the problem be parallelized?)</li>
<li>Models and Simulation (how is the model formulated)</li>
</ul>
<h4 id="principles-of-parallel-computing">Principles of parallel computing</h4>
<p>Parallel applications contain of multiple computational tasks which are run concurrently.
With parallel computing we want to use local data (in the caches) more, for better locality. The task size is also very
important (what parallelism granularity?), as well as scheduling and load balancing across the different processors and
tasks.</p>
<h4 id="performance-metrics">Performance Metrics</h4>
<p>Important metrics with HPC:</p>
<ul>
<li><strong>Execution Time</strong></li>
<li><strong>Speed-up</strong> typically compared to sequential</li>
<li><strong>Utilization</strong> $time * \#\text{CPU}$ in use</li>
</ul>
<h4 id="parallelism">Parallelism</h4>
<p>All programs have an intrinsically sequential part which cannot be parallelized. Therefore with Amdahl&rsquo;s law we can find
the maximal speedup of the program. However, the speedup is bound by the sequential part of the program. With $s$ being
the sequential part, $p$ the number of processors and $S$ the speedup, we have</p>
<p>$$S=T_{seq}/T_{par}=\frac{1}{s+\frac{(1-s)}{p}}\le \frac{1}{s}$$</p>
<p>where $1-s$ is the parallelizable part of the program.</p>
<h5 id="data-locality">Data Locality</h5>
<p>Algorithms should be implemented to enable smart data distribution which results in best data locality for quick
accesses. If locality is not possible other techniques such as prefetching are used.</p>
]]></content></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/info/</link><pubDate>Sun, 29 Aug 2021 21:53:20 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/info/</guid><description>IN4049TU Introduction to High Performance Computing: Studyguide: # TODO: FILL IN STUDYGUIDE LINK
Book: TODO: FILL IN THE BOOK FOR THE COURSE
All content and images are based on and retrieved from the lecture slides and/or the previously mentioned book and/or any other content that is provided in the course.
Authors Nick Tehrany</description><content type="html"><![CDATA[<h2 id="in4049tu-introduction-to-high-performance-computing">IN4049TU Introduction to High Performance Computing:</h2>
<p>Studyguide: <strong># TODO: FILL IN STUDYGUIDE LINK</strong></p>
<p>Book: <strong>TODO: FILL IN THE BOOK FOR THE COURSE</strong></p>
<p>All content and images are based on and retrieved from the lecture slides and/or the previously mentioned book and/or any
other content that is provided in the course.</p>
<h3 id="authors">Authors</h3>
<ul>
<li><a href="https://github.com/nicktehrany">Nick Tehrany</a></li>
</ul>
]]></content></item></channel></rss>