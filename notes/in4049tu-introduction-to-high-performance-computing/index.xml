<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>IN4049TU Introduction to High Performance Computing on Nick Tehrany</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/</link><description>Recent content in IN4049TU Introduction to High Performance Computing on Nick Tehrany</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 29 Aug 2021 21:53:20 +0200</lastBuildDate><atom:link href="https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/index.xml" rel="self" type="application/rss+xml"/><item><title>Machine Learning</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/machine-learning/</link><pubDate>Sun, 23 Jan 2022 15:01:11 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/machine-learning/</guid><description/><content type="html"></content></item><item><title>Matrix Multiply</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/matrix-multiply/</link><pubDate>Sun, 23 Jan 2022 13:55:10 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/matrix-multiply/</guid><description>The main cost of any parallel algorithm is, apart from the arithmetic computations, the communication. This includes the moving of data (e.g. from memory to caches) or over network communication between nodes. These are often the main bottleneck of parallel applications.</description><content type="html">&lt;p>The main cost of any parallel algorithm is, apart from the arithmetic computations, the communication. This includes the moving of data (e.g. from memory to caches) or over network communication between nodes. These are often the main bottleneck of parallel applications.&lt;/p></content></item><item><title>N-Body</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/n-body/</link><pubDate>Sun, 23 Jan 2022 13:21:28 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/n-body/</guid><description>N-Body simulations approximate the evolution of a system numerically. In this each body of the system constantly interacts with every other body.
The bottleneck within this if particles depend on all other particles. If they just depend on some external factor or close neighbors, this can easily be done but all other particles means for every particle you have to loop through all other particles which is a heavy overhead ($O(N^2)$.</description><content type="html"><![CDATA[<p>N-Body simulations approximate the evolution of a system numerically. In this each body of the system constantly interacts with every other body.</p>
<p>The bottleneck within this if particles depend on all other particles. If they just depend on some external factor or close neighbors, this can easily be done but all other particles means for every particle you have to loop through all other particles which is a heavy overhead ($O(N^2)$.</p>
<p>An approach to improve this is with divide-and-conquer algorithms that only account for neighbors that are close enough to a certain particle instead of the entire system. While this means that there will be less computing, it can be inaccurate or even incorrect (depending on the application). A so called <strong>box</strong> is then a collection of particles that will be represented as one, and the <strong>radius</strong> $r$ is the distance from the current particle to the center of that box (Center of Mass, CM).</p>
<p>From this one can then construct a <strong>Quad Tree</strong> where each non-leaf node has 4 children (in a complete quad tree, otherwise it can have less), each node (depicting the box has more boxes in it). Similar to this you can have <em>oct trees</em> with up to 8 children per node.</p>
<p>Since such complete tree would waste space, as there are not the same number of particles in each box, and therefore a lot of the children nodes will be empty, there are <strong>adaptive quad trees</strong> where empty nodes are excluded in the tree.</p>
<h2 id="barnes-hut-algorithm">Barnes-Hut Algorithm</h2>
<p>It uses the quad tree and then for each node calculates the CM of all its particles (so it&rsquo;s children) then uses this to compute forces of sub-squares. Complexity is $O(N \textmd{log} N)$ just like the prior tree methods.</p>
<h2 id="fast-multipole-method-fmm">Fast Multipole Method (FMM)</h2>
<p>Instead of BH it includes more info than the center mass and total mass at each box, which makes it more accurate but more expensive. However, to compensate for this it only accesses a fixed set of boxes at each level in the tree. The subset of the quad tree that a local processor needs (in the parallel running of it) is called the <strong>Locally Essential Tree (LET)</strong>. For each LET the force calculations can then be done in parallel.</p>
]]></content></item><item><title>Graph Partitioning</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/graph-partitioning/</link><pubDate>Sat, 22 Jan 2022 19:10:04 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/graph-partitioning/</guid><description>The goal with graph partitioning is that the entire graph is partitioned into even parts (about the same size/weight associated to the nodes/edged). However, choosing an optimal partitioning is NP-complete (can be proved that this is just as difficult as other hard problems in Nondeterministic Polynomial time). Therefore the only algorithms for partitioning have exponential cost on the number of nodes in the graph. This is why a heuristical approach is needed.</description><content type="html"><![CDATA[<p>The goal with graph partitioning is that the entire graph is partitioned into even parts (about the same size/weight associated to the nodes/edged). However, choosing an optimal partitioning is NP-complete (can be proved that this is just as difficult as other hard problems in Nondeterministic Polynomial time). Therefore the only algorithms for partitioning have exponential cost on the number of nodes in the graph. This is why a heuristical approach is needed.</p>
<h2 id="partitioning-with-nodal-coordinates">Partitioning with Nodal Coordinates</h2>
<h3 id="inertial-partitioning">Inertial Partitioning</h3>
<p>A 2D graph, pick a line that has half the nodes on one side and the other on the other side (for other dimensions it&rsquo;s 2D plane, etc.). Then a perpendicular line to that line is chosen and each point is projected onto that lean, with which now the median to partition the nodes by minimizing the sum of the squares of each node to the line L. The resulting line is the total least squares of all the points.</p>
<h2 id="partitioning-without-nodal-coordinates">Partitioning without Nodal Coordinates</h2>
<h3 id="breadth-first-search-bfs">Breadth First Search (BFS)</h3>
<p>From a graph a subgraph $T$ with root $r$ is produced and each node is assigned a level which is its distance to the root. Then using a queue going up in the levels (visually from root going down but level id increases) and have a queue for processed nodes. For the current node, for all it&rsquo;s unmarked children (hasn&rsquo;t been processed) add the child to the new tree $T$ as node $N_T$. Also add the edge to the tree as $E_T$. Then add the this child to the queue for processing, mark that child as having been processed (this happens in for loop for all unmarked children, so first process all unmarked children then later their children). The queue is FIFO and the entire thing runs in a loop while the queue is not empty,</p>
<h3 id="coordinate-free-kernighanlin">Coordinate Free: Kernighan/Lin</h3>
<p>Given a graph, iteratively improve it by picking a new partition that splits the graph into two even halves $A$ and $B$, then find a subset of each of the splits $X$ from $A$ and $Y$ from $B$ such that they are even $|X|=|Y|$. If the cost of the two is lower when swapping the two swap them (cost is summation of edge weights)</p>
<h3 id="spectral-bisection">Spectral Bisection</h3>
<p>Using an Incidence matrix, we can show with it the graph connection by having in the Incidence matrix the connections depicted (1 for source -1 at the sink) vertex (each row in Incidence matrix is an edge and column is a vertex). Based on the Incidence matrix we can generate the Graph Laplacian as the transpose of the Incidence matrix time the Incidence matrix ($C^T * C$).</p>
<h2 id="multilevel-partitioning">Multilevel Partitioning</h2>
<p>Similar to the previous methods for multigrids, here also replace the graph by a coarse approximation and partition that approximated graph. That partition we can then iteratively improve. Coarsening a graph can be done with maximal matching, which is a greedy approach that checks each node and matches them such that there are no two edges that share an endpoint.</p>
]]></content></item><item><title>Performance Analysis</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/performance-analysis/</link><pubDate>Sat, 22 Jan 2022 18:15:51 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/performance-analysis/</guid><description>Metrics We have the following
$T_S$ is the Serial execution time, or the part that is done sequentially and how long that takes $T_P$ the parallel execution time, $T_O$ The overhead, given as %T_O = p * T_P - T_S$ with $p$ parallel processes. $T_O$ is equal to 0 in the optimal case. Speedup is given as $S=\frac{T_{serial_best}}{T_P}$ Then we have efficiency given as multiple equivalent options
$$E=\frac{S}{p}=\frac{T_S}{p * T_P}=\frac{1}{1+\frac{T_O}{T_S}}$$</description><content type="html"><![CDATA[<h3 id="metrics">Metrics</h3>
<p>We have the following</p>
<ul>
<li>$T_S$ is the Serial execution time, or the part that is done sequentially and how long that takes</li>
<li>$T_P$ the parallel execution time,</li>
<li>$T_O$ The overhead, given as %T_O = p * T_P - T_S$ with $p$ parallel processes. $T_O$ is equal to 0 in the optimal case.</li>
<li>Speedup is given as $S=\frac{T_{serial_best}}{T_P}$</li>
</ul>
<p>Then we have efficiency given as multiple equivalent options</p>
<p>$$E=\frac{S}{p}=\frac{T_S}{p * T_P}=\frac{1}{1+\frac{T_O}{T_S}}$$</p>
<p>Efficiency is simply just the achieved unit divided by the optimal unit (this gives the percentage of what you can maximally achieve).</p>
<p>We also have for simple performance measuring</p>
<ul>
<li>$T_S = number_ops * t_{op}$</li>
<li>$T_P = (number_ops / p) * t_{op}+T_{comm}$</li>
<li>$T_{comm} = number_comm * t{comm}$ + an additional overhead for setting up communication each time</li>
</ul>
<p>The <strong>arithmetic intensity</strong> is given as the number of arithmetic floating point operations that are executed per byte of memory accessed. <strong>Operational intensity</strong> similarly gives the number of operations per byte of memory accessed.</p>
<p>The <strong>attainable performance</strong> is given as <em>min(Peak floating point performance, peak memory BW * operation intensity)</em>.</p>
]]></content></item><item><title>Multigrid Methods</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/multigrid-methods/</link><pubDate>Sat, 22 Jan 2022 17:54:50 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/multigrid-methods/</guid><description>The goal of multigrid methods is that with the prior methods such as Jacobi, SOR, information has to move across the grid, whereas with multigrid methods, we approximate on a coarser grid, solve the problem on there and then use that solution to start on finer grid problems. Coarser grids work recursively by using an even coarser grid. If the coarse grid solution is a good approximation for the fine grid, the final solution will be successful.</description><content type="html"><![CDATA[<p>The goal of multigrid methods is that with the prior methods such as Jacobi, SOR, information has to move across the grid, whereas with multigrid methods, we approximate on a coarser grid, solve the problem on there and then use that solution to start on finer grid problems. Coarser grids work recursively by using an even coarser grid. If the coarse grid solution is a good approximation for the fine grid, the final solution will be successful.</p>
<h2 id="multigrid-v-cycle">Multigrid V-Cycle</h2>
<p>Solves its initial equation $t_i x_i = b_i$ with a $b_i$ given and a guess for $x_i$. If it was a good guess return, else improve the solution with finer grid.</p>
<p>You therefore start with a fine grid, iterate on it, include this into a coarser grid and then iterate on the coarser grid. Then we go back to finer grid (which is where the V shape for its name comes from). The including into a finer grid is done by using the residual which depicts how the current matrix (result) is not satisfying the solution (since we don&rsquo;t have the error but the results for the current matrix).</p>
]]></content></item><item><title>Parallel Programming on GPUs</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming-on-gpus/</link><pubDate>Sat, 22 Jan 2022 17:13:01 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming-on-gpus/</guid><description>Parallel Computing on Graphical Processing Units Graphics cards are often used for scientific computing using SIMD (single Instruction Multiple Data) and providing parallelism in the cards with threads. GPUs offer cheap hardware that is capable of running thousands of threads at once, and it is better scalable than MPI due to the significantly lower communication overhead. However, GPUs only have limited available memory (often without ECC) and were complicated to program.</description><content type="html"><![CDATA[<h2 id="parallel-computing-on-graphical-processing-units">Parallel Computing on Graphical Processing Units</h2>
<p>Graphics cards are often used for scientific computing using SIMD (single Instruction Multiple Data) and providing parallelism in the cards with threads. GPUs offer cheap hardware that is capable of running thousands of threads at once, and it is better scalable than MPI due to the significantly lower communication overhead. However, GPUs only have limited available memory (often without ECC) and were complicated to program.</p>
<h3 id="cuda">CUDA</h3>
<p>CUDA (Compute Unified Device Architecture) make this easier (for Nvidia) by providing GPU programming similar to C with library routines, language extensions such as codewords, etc.
This is similar to OpenCL (Open Computing Language) which was used for parallel kernel programming, however OpenCL is only for GPUs but it is open source unlike CUDA being Nvidia built.</p>
<h2 id="gpus">GPUs</h2>
<p>GPUs are best utilized for Matrix products and not so good for matrix-vector products.</p>
<p>GPUs can have 1 or more devices -&gt; which have several streamprocessors (SM) -&gt; which have up to 32 active concurrent threads (called <strong>warps</strong>). A <strong>block</strong> is the set of threads that running on single SM. The SM can run up to 32 threads in parallel and up to 768 or 1536 sequential.</p>
<p>Devices have <strong>global memory</strong> that all streaming multiproccessors can access and which is persistent. A <strong>constant and texture memory</strong> that can only be <strong>read</strong> and which also have a local cache on each SM. The SM then each has a shared memory for its threads.</p>
]]></content></item><item><title>Poisson Equation</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/poisson-equation/</link><pubDate>Fri, 21 Jan 2022 21:23:46 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/poisson-equation/</guid><description>Iterative Methos for Linear Systems Iterations methods are used for solving linear equations (often heat problems, air flow for planes, etc.) with methods such as Jacobi Iteration, Gauss-Seidel iteration and SOR (successive over-relaxation). These iterative methods generate a sequence of approximation vectors that converge to a solution. Evaluations then show how quickly this iteration sequence converges. With Jacobi and Gauss-Seidel, the computation of a new approximation depends on a combination of prior computed approximated vectors, which is why they are called relaxation methods.</description><content type="html"><![CDATA[<h2 id="iterative-methos-for-linear-systems">Iterative Methos for Linear Systems</h2>
<p>Iterations methods are used for solving linear equations (often heat problems, air flow for planes, etc.) with methods such as Jacobi Iteration, Gauss-Seidel iteration and SOR (successive over-relaxation). These iterative methods generate a sequence of approximation vectors that converge to a solution. Evaluations then show how quickly this iteration sequence converges. With Jacobi and Gauss-Seidel, the computation of a new approximation depends on a combination of prior computed approximated vectors, which is why they are called <em>relaxation methods</em>.</p>
<p>Convergence for Jacobi and Gauss-Seidel occurs when the matrix (approximation result) is strongly diagonal dominant, which means the absolute values of the diagonal of the elements in the matrix are significantly larger than that the absolute value of the sums of other rows.</p>
<p>Since often convergence with these two methods is not fast enough, an additional <em>relaxation parameter</em> is added. This is what <strong>Jacobi over-relaxation</strong> is based on. With it the matrix is split with the additional relaxation parameter.</p>
<p><strong>Successive over-relaxation</strong> is Gauss-Seidel with the relaxation parameter. The result from the prior approximation is combined with the elements in the current one to give a new relaxation parameter for each iteration.</p>
<h2 id="parallel-implementation-of-jacobi-iteration-method">Parallel Implementation of Jacobi Iteration Method</h2>
<p>Since each element in the approximation can be calculated individually, we can have a maximum parallelism here of the number of approximation elements. However, since approximations of each element require those of the previous approximation there is a need for some communication.</p>
<h2 id="parallel-implementation-of-gauss-seidel-iteration-method">Parallel Implementation of Gauss-Seidel Iteration Method</h2>
<p>Here all elements in the approximation depend on the prior element in that approximation instead of the prior approximation. This means that this cannot be parallelized, but only the part of actually calculating the element (the scalar for that approximation).</p>
<h2 id="red-black-ordering">Red-black ordering</h2>
<p>Since there is only limited parallelism, there needs to be a reordering that splits the 2D mesh (of grid points that have to exchange approximation elements) into red and black pairs like a checkered board. This way all points can do independent calculations then switch to the other color to do their independent operations (however they then depend on the prior color, which is why always one color is active).</p>
<h2 id="summary">Summary</h2>
<p>For Jacobi SOR and CG it takes $n=N^\frac{1}{2}$ steps with an $nxn$ matrix for the info to have traveled across the matrix.</p>
<h2 id="metrics">Metrics</h2>
<p>Data Locality Ratio is given as</p>
<p>$$\frac{T_{computed}}{T_{communicated}}$$</p>
<p>Communication time is given as, with $L$ message length in bytes, $Latency=Startup$ and $B$ being bandwidth</p>
<p>$$T_{comm}(L)=Startup+\frac{L}{B}$$</p>
<h2 id="pram">PRAM</h2>
<p>Shows a simplified abstraction of parallel hardware, where there are e.g. no communication overheads for memory accesses.
It has four variants:</p>
<ol>
<li><strong>Exclusive-read, Exclusive-write (EREW)</strong>: Each processor can only read and write to its memory not any other belonging to other processors.</li>
<li><strong>Concurrent-read, Exclusive-write (CREW)</strong>: Processors can read from any memory, but can only write to their own.</li>
<li><strong>Exclusive-read, Concurrent-write (ERCW)</strong>: Processors can only read from their own memory but can write to any other belonging to any other processor.</li>
<li><strong>Concurrent-read, Concurrent-write (CRCW)</strong>: Processors can both read and write to any memory.</li>
</ol>
]]></content></item><item><title>Parallel Programming</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming/</link><pubDate>Tue, 14 Sep 2021 15:50:41 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming/</guid><description>For distributed memory MPI is used as programming model, with shared memory OpenMP is used, and for GPUs CUDA is used.
MPI Program With MPI programs, the processes that execute the program all have their own local data. Typically one process will be on one core, each process can then access its own data locally and use message passing to get other data from other cores. In MPI programming the environment needs to be initialized before it can be used, and finalized when it&amp;rsquo;s done.</description><content type="html"><![CDATA[<p>For distributed memory MPI is used as programming model, with shared memory OpenMP is used, and for GPUs CUDA is used.</p>
<h3 id="mpi-program">MPI Program</h3>
<p>With MPI programs, the processes that execute the program all have their own local data. Typically one process will be on one core, each process can then access its own data locally and use message passing to get other data from other cores. In MPI programming the environment needs to be initialized before it can be used, and finalized when it&rsquo;s done. <code>numberOfProcs</code> is set to the total number of processes (e.g. with 4 processes this is 4), and <code>rank</code> is the rank of the process, this starts at 0 (!!), so the first process will have rank 0, second will have rank 2, and so forth.</p>
<h3 id="point-to-point-communication">Point-to-Point Communication</h3>
<p>For enabling communication for processes that don&rsquo;t share memory.</p>
<p><code>MPI_Send</code> and <code>MPI_Recv</code> are both blocking calls, therefore deadlocks can occur and should be avoided.</p>
<p>MPI Datatypes are used to tell it how to interpret binary values from the send buffer/receive buffer, convert them etc. It has a bunch of predefined datatypes such as <code>MPI_CHAR</code> or <code>MPI_INT</code>.</p>
<p>MPI also has <code>MPI_Sendrecv</code> which sends one message and receives another in any order, and buffers (send, and recv buffers) have to different and cannot overlap. There is also the <code>MPISendrecv_replace</code> the first sends the message then receives using the same buffer.</p>
<p><code>MPI_Isend</code> and <code>MPI_Irecv</code> are the non-blocking counterparts to the prior mentioned ones. One could then also use the <code>MPI_Wait</code> to block until it&rsquo;s completed, typically this is done after some work is completed and the process needs the data from the recv.</p>
<p>Four send modes in MPI:</p>
<ul>
<li>Standard mode: Block until the message is fully transferred.</li>
<li>Synchronous mode: Block until a reply (the recv) from the receiver has arrived.</li>
<li>Buffered mode: Block until the message (to be sent) is copied to a buffer (specified by user)</li>
<li>Ready mode: If a receive exists only then does this succeed. Not recommended to be used</li>
</ul>
]]></content></item><item><title>Parallel and Distributed Architecture</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-and-distributed-architecture/</link><pubDate>Tue, 07 Sep 2021 16:56:12 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-and-distributed-architecture/</guid><description>Memory Classifications Based on memory organization:
SM - Shared Memory DM - Distributed Memory Based on access times:
UMA - Uniform Memory Access NUMA - Non-Uniform Memory Access SMP - Symmetric Multi-Processor Based on data and control flows with Flynn&amp;rsquo;s taxonomy:
SISD - Single Instruction Single Data MISD - Multiple Instruction Single Data SIMD - Single Instruction Multiple Data MIMD - Multiple Instruction Multiple Data Extensions of memory systems:</description><content type="html"><![CDATA[<h4 id="memory-classifications">Memory Classifications</h4>
<p>Based on memory organization:</p>
<ul>
<li><strong>SM</strong> - Shared Memory</li>
<li><strong>DM</strong> - Distributed Memory</li>
</ul>
<p>Based on access times:</p>
<ul>
<li><strong>UMA</strong> - Uniform Memory Access</li>
<li><strong>NUMA</strong> - Non-Uniform Memory Access</li>
<li><strong>SMP</strong> - Symmetric Multi-Processor</li>
</ul>
<p>Based on data and control flows with <strong>Flynn&rsquo;s taxonomy</strong>:</p>
<ul>
<li><strong>SISD</strong> - Single Instruction Single Data</li>
<li><strong>MISD</strong> - Multiple Instruction Single Data</li>
<li><strong>SIMD</strong> - Single Instruction Multiple Data</li>
<li><strong>MIMD</strong> - Multiple Instruction Multiple Data</li>
</ul>
<p>Extensions of memory systems:</p>
<ul>
<li>Shared cache, where processors share caches and memory.</li>
<li>Bus-based shared memory (Symmetric Multiprocessors - SMP), where processors share the same memory and bus to it.</li>
</ul>
]]></content></item><item><title>Introduction</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/introduction/</link><pubDate>Tue, 31 Aug 2021 16:27:34 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/introduction/</guid><description>Why do we need powerful computer systems? Because more and more computation is moving to computer systems, experiments, proofs and other theoretical engineering done traditionally on paper. Said engineering is too slow on paper and computationally limited, as well as dangerous sometimes (in scenarios where simulation is needed, e.g. earthquakes, car crashes).
Computational Science Triangle presents the combination of computer systems, applications and algorithms, out of which a model is then constructed.</description><content type="html"><![CDATA[<h4 id="why-do-we-need-powerful-computer-systems">Why do we need powerful computer systems?</h4>
<p>Because more and more computation is moving to computer systems, experiments, proofs and other theoretical engineering
done traditionally on paper. Said engineering is too slow on paper and computationally limited, as well as dangerous
sometimes (in scenarios where simulation is needed, e.g. earthquakes, car crashes).</p>
<p><strong>Computational Science Triangle</strong> presents the combination of computer systems, applications and algorithms, out of
which a model is then constructed. This model is then used to fulfill the computational science paradigm of the
previously mentioned theoretical engineering scenarios.</p>
<p>Three new kinds of HPC have emerged with the need for Big Data:</p>
<ul>
<li><strong>Volume</strong> - large volumes of data being processed</li>
<li><strong>Velocity</strong> - new data is being generated quickly</li>
<li><strong>Variety</strong> - there are a large variety of data formats</li>
</ul>
<p>Additionally, HPC systems require large amounts of memory, as the data is stored in memory for processing.</p>
<p>HPC is also widely used for web applications, such as web crawling, indexing, and sorting (e.g. with MapReduce).</p>
<h4 id="requirements-of-hpc">Requirements of HPC</h4>
<p>Modern HPC relies on:</p>
<ul>
<li>Powerful supercomputers (enough resources to do the computation)</li>
<li>Parallelism in the problem (how far can the problem be parallelized?)</li>
<li>Models and Simulation (how is the model formulated)</li>
</ul>
<h4 id="principles-of-parallel-computing">Principles of parallel computing</h4>
<p>Parallel applications contain of multiple computational tasks which are run concurrently.
With parallel computing we want to use local data (in the caches) more, for better locality. The task size is also very
important (what parallelism granularity?), as well as scheduling and load balancing across the different processors and
tasks.</p>
<h4 id="performance-metrics">Performance Metrics</h4>
<p>Important metrics with HPC:</p>
<ul>
<li><strong>Execution Time</strong></li>
<li><strong>Speed-up</strong> typically compared to sequential</li>
<li><strong>Utilization</strong> $time * \#\text{CPU}$ in use</li>
</ul>
<h4 id="parallelism">Parallelism</h4>
<p>All programs have an intrinsically sequential part which cannot be parallelized. Therefore with Amdahl&rsquo;s law we can find
the maximal speedup of the program. However, the speedup is bound by the sequential part of the program. With $s$ being
the sequential part, $p$ the number of processors and $S$ the speedup, we have</p>
<p>$$S=T_{seq}/T_{par}=\frac{1}{s+\frac{(1-s)}{p}}\le \frac{1}{s}$$</p>
<p>where $1-s$ is the parallelizable part of the program.</p>
<h5 id="data-locality">Data Locality</h5>
<p>Algorithms should be implemented to enable smart data distribution which results in best data locality for quick
accesses. If locality is not possible other techniques such as prefetching are used.</p>
]]></content></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/info/</link><pubDate>Sun, 29 Aug 2021 21:53:20 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/info/</guid><description>IN4049TU Introduction to High Performance Computing: Studyguide: # TODO: FILL IN STUDYGUIDE LINK
Book: TODO: FILL IN THE BOOK FOR THE COURSE
All content and images are based on and retrieved from the lecture slides and/or the previously mentioned book and/or any other content that is provided in the course.
Authors Nick Tehrany</description><content type="html"><![CDATA[<h2 id="in4049tu-introduction-to-high-performance-computing">IN4049TU Introduction to High Performance Computing:</h2>
<p>Studyguide: <strong># TODO: FILL IN STUDYGUIDE LINK</strong></p>
<p>Book: <strong>TODO: FILL IN THE BOOK FOR THE COURSE</strong></p>
<p>All content and images are based on and retrieved from the lecture slides and/or the previously mentioned book and/or any
other content that is provided in the course.</p>
<h3 id="authors">Authors</h3>
<ul>
<li><a href="https://github.com/nicktehrany">Nick Tehrany</a></li>
</ul>
]]></content></item></channel></rss>