<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Nick Tehrany</title><link>https://nicktehrany.github.io/</link><description>Recent content on Nick Tehrany</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 22 Jan 2022 19:10:04 +0100</lastBuildDate><atom:link href="https://nicktehrany.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Graph Partitioning</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/graph-partitioning/</link><pubDate>Sat, 22 Jan 2022 19:10:04 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/graph-partitioning/</guid><description>The goal with graph partitioning is that the entire graph is partitioned into even parts (about the same size/weight associated to the nodes/edged). However, choosing an optimal partitioning is NP-complete (can be proved that this is just as difficult as other hard problems in Nondeterministic Polynomial time). Therefore the only algorithms for partitioning have exponential cost on the number of nodes in the graph. This is why a heuristical approach is needed.</description></item><item><title>Performance Analysis</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/performance-analysis/</link><pubDate>Sat, 22 Jan 2022 18:15:51 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/performance-analysis/</guid><description>Metrics We have the following
$T_S$ is the Serial execution time, or the part that is done sequentially and how long that takes $T_P$ the parallel execution time, $T_O$ The overhead, given as %T_O = p * T_P - T_S$ with $p$ parallel processes. $T_O$ is equal to 0 in the optimal case. Speedup is given as $S=\frac{T_{serial_best}}{T_P}$ Then we have efficiency given as multiple equivalent options
$$E=\frac{S}{p}=\frac{T_S}{p * T_P}=\frac{1}{1+\frac{T_O}{T_S}}$$</description></item><item><title>Multigrid Methods</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/multigrid-methods/</link><pubDate>Sat, 22 Jan 2022 17:54:50 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/multigrid-methods/</guid><description>The goal of multigrid methods is that with the prior methods such as Jacobi, SOR, information has to move across the grid, whereas with multigrid methods, we approximate on a coarser grid, solve the problem on there and then use that solution to start on finer grid problems. Coarser grids work recursively by using an even coarser grid. If the coarse grid solution is a good approximation for the fine grid, the final solution will be successful.</description></item><item><title>Parallel Programming on GPUs</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming-on-gpus/</link><pubDate>Sat, 22 Jan 2022 17:13:01 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming-on-gpus/</guid><description>Parallel Computing on Graphical Processing Units Graphics cards are often used for scientific computing using SIMD (single Instruction Multiple Data) and providing parallelism in the cards with threads. GPUs offer cheap hardware that is capable of running thousands of threads at once, and it is better scalable than MPI due to the significantly lower communication overhead. However, GPUs only have limited available memory (often without ECC) and were complicated to program.</description></item><item><title>Poisson Equation</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/poisson-equation/</link><pubDate>Fri, 21 Jan 2022 21:23:46 +0100</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/poisson-equation/</guid><description>Iterative Methos for Linear Systems Iterations methods are used for solving linear equations (often heat problems, air flow for planes, etc.) with methods such as Jacobi Iteration, Gauss-Seidel iteration and SOR (successive over-relaxation). These iterative methods generate a sequence of approximation vectors that converge to a solution. Evaluations then show how quickly this iteration sequence converges. With Jacobi and Gauss-Seidel, the computation of a new approximation depends on a combination of prior computed approximated vectors, which is why they are called relaxation methods.</description></item><item><title>Parallel Programming</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming/</link><pubDate>Tue, 14 Sep 2021 15:50:41 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-programming/</guid><description>For distributed memory MPI is used as programming model, with shared memory OpenMP is used, and for GPUs CUDA is used.
MPI Program With MPI programs, the processes that execute the program all have their own local data. Typically one process will be on one core, each process can then access its own data locally and use message passing to get other data from other cores. In MPI programming the environment needs to be initialized before it can be used, and finalized when it&amp;rsquo;s done.</description></item><item><title>Parallel and Distributed Architecture</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-and-distributed-architecture/</link><pubDate>Tue, 07 Sep 2021 16:56:12 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/parallel-and-distributed-architecture/</guid><description>Memory Classifications Based on memory organization:
SM - Shared Memory DM - Distributed Memory Based on access times:
UMA - Uniform Memory Access NUMA - Non-Uniform Memory Access SMP - Symmetric Multi-Processor Based on data and control flows with Flynn&amp;rsquo;s taxonomy:
SISD - Single Instruction Single Data MISD - Multiple Instruction Single Data SIMD - Single Instruction Multiple Data MIMD - Multiple Instruction Multiple Data Extensions of memory systems:</description></item><item><title>Introduction</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/introduction/</link><pubDate>Tue, 31 Aug 2021 16:27:34 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/introduction/</guid><description>Why do we need powerful computer systems? Because more and more computation is moving to computer systems, experiments, proofs and other theoretical engineering done traditionally on paper. Said engineering is too slow on paper and computationally limited, as well as dangerous sometimes (in scenarios where simulation is needed, e.g. earthquakes, car crashes).
Computational Science Triangle presents the combination of computer systems, applications and algorithms, out of which a model is then constructed.</description></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/info/</link><pubDate>Sun, 29 Aug 2021 21:53:20 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4049tu-introduction-to-high-performance-computing/content/info/</guid><description>IN4049TU Introduction to High Performance Computing: Studyguide: # TODO: FILL IN STUDYGUIDE LINK
Book: TODO: FILL IN THE BOOK FOR THE COURSE
All content and images are based on and retrieved from the lecture slides and/or the previously mentioned book and/or any other content that is provided in the course.
Authors Nick Tehrany</description></item><item><title>Useful Commands</title><link>https://nicktehrany.github.io/posts/useful-commands/</link><pubDate>Sat, 17 Jul 2021 14:02:11 +0200</pubDate><guid>https://nicktehrany.github.io/posts/useful-commands/</guid><description>This is not necessarily a blog post but rather a handy spot of where I keep some of the useful commands and shortcuts I use.
gist with all useful commands that I use or need and tend to forget since it&amp;rsquo;s just too many, a lot of these are custom key mappings so they might not work by default.
Vim some vim commands
Ctrl + c # compile latex file Ctrl + l # clear output log from latex compile Ctrl + w arrow # change window to arrow direction Ctrl + b # open NerdTree Ctrl + r # refresh NerdTree :sp # split horizontally :vsp # split vertically # Navigation gg # jump to top of file G # jump to end of file ZZ # save and quit ZQ # quit \tab # switch to next tab in buffer \ Shift tab # swithc to previous tab in buffer / # search use N and Shift + N to navigate them w # goto next word b # goto previous word { # goto previous paragraph } # goto next paragraph 0 # goto beginning of line $ # goto end of line Ctrl + ] # goto function def (requires ctags and other plugins installed) Ctrl + o # go back to previous location u # or U inside Nerdtree to up a dir # Editing Ctrl + w # delete word a # insert mode after character o # insert mode after line dd # delete Line D # delete part of line after cursor cw # delete next word and enter insert mode di( # delete everything inside (), leave the parenthesis da( # delete everything inside (), including the parenthesis gq$ # wrap words until end of line (or gq0 for until beginning of line) :m +1 # move line 1 down (- for up and line numbers) if no plus line number is absolute Zathura K # Zoom in J # Zoom out t # goto top u # go half page up d # go half page down i # invert colors D # change page mode (dual/single) Shell Window Tiling Super + / # go through applications Super + y # Tile windows Super + o # change orientation Super + enter # adjustment mode arrow # move window Ctrl + arrow # swap windows Shift + arrow # resize Super + Shift + down # move window down one workspace Super + Shift + right # move window to right monitor Super + s # stacking mode Super + g # floating mode Key Remapping I use these to remap CAPS lock to esc and home to delete.</description></item><item><title>Deterministic DBs</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/deterministic-dbs/</link><pubDate>Mon, 21 Jun 2021 13:25:36 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/deterministic-dbs/</guid><description>Calvin is a transactional scheduling and data replication layer that runs alongside non-transactional storage systems. It uses a deterministic locking protocol (DPL) which eliminates the need of distributed commit protocols (e.g. 2PC), it also uses a shared nothing architecture. The model it uses is a deterministic database, which works by knowing the exact read and write sets (which keys are going to be accessed) before starting the transaction. Then if we know what replicas a transaction will touch, the individual operations can be sent to the replica individually and be serialized that way.</description></item><item><title>Google Spanner: Google's Globally Distributed Database</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/googlespanner/</link><pubDate>Wed, 09 Jun 2021 14:52:13 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/googlespanner/</guid><description>Spanner provides a unique feature of providing distributed transactions without locks, through the property of Spanner that distributed transactions are externally consistent. Instead of locking the database for reads to get the data (all writes are locked), it works by generating a snapshot of the database from which the reads happen (writes can then still happen on the original database), however now snapshots need to be synchronized across the distributed shards of the database.</description></item><item><title>Paxos Megastore</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/paxosmegastore/</link><pubDate>Tue, 25 May 2021 17:05:50 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/paxosmegastore/</guid><description>Paxos is a distributed systems consensus algorithm that can tolerate non-malicious failures. It provides safety that only a value that has been proposed by some node will be the chosen one, only a single value is chosen, and only chosen values are communicated further to all nodes. Liveness is provided by guaranteeing that some proposed value will eventually be chosen, and that if a value has been chosen, every node will eventually know about this choice.</description></item><item><title>Streams</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/streams/</link><pubDate>Mon, 24 May 2021 18:22:43 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/streams/</guid><description>Data can be static (it does not change, is written once and never updated), therefore most commonly used is streaming data, which changes (changes at some point in time, does not have to be frequently or soon but it will eventually change) and is updated. A data stream is a stream of events with each element having a timestamp.
Types of data:
Bounded data: a dataset with a beginning and an end.</description></item><item><title>Bigtable</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/bigtable/</link><pubDate>Wed, 19 May 2021 16:03:09 +0200</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/bigtable/</guid><description>Bigtable is a high-performance proprietary database which is used by multiple google services (or was used, has several newer versions mostly replacing it). It is meant for large throughput with very large files. Requirements are that processes update continuously and asynchronously. Additionally, it is required to be fault tolerant, persistent, and can use and work efficiently on cheap hardware, and scale well.
Google File System (GFS) A file system for very large files.</description></item><item><title>Parallel Analytical Processing</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/parallel_analytical_processing/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/parallel_analytical_processing/</guid><description>Relational DBs vs. Modern Big Data Processing Stack Relational dbs such as SQL go through a stack that starts with the SQL compiler &amp;amp; optimizer and then executes them on a relational dataflow engine, on top of the transaction manager, which then is on top of the storage and the OS. Big data on the other hand, also has the SQL compiler &amp;amp; optimizer which then runs of top of different dataflow engines (Flink, Spark, Hadoop).</description></item><item><title>Scalable Data Integration and Discovery in Data Lakes</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/datalake/</link><pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/datalake/</guid><description>Data Lakes The problem starts in the management of big data, which has the following requirements:
Volume: the data is typically large Variety: the type of the data varies Velocity: the data needs to be accessed and processed quickly Veracity: the quality of the data needs to be maintained and guaranteed Value: the data is used for intended purposes and holds value to that purpose Data lakes are a way of storing large amounts of data without strict structure and guidelines, and all data is just &amp;ldquo;dumped&amp;rdquo; there.</description></item><item><title>NoSQL - Design Principles</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/no_sql_design/</link><pubDate>Thu, 29 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/no_sql_design/</guid><description>Replication Master-Slave Replication All updates are made directly to the master which then propagates the changes to all the slaves.
P2P Replication Replication without a master node, now updates can be on any of the replicas which then propagate the updates to all the replicas.
Distributed Transactions Two Phase Commit (2PC) Transactions are submitted to a coordinator which then sends a prepare to all workers. The workers log the request and respond to the coordinator.</description></item><item><title>P2P Storage</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/p2p/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/p2p/</guid><description>Databases have some sort of model to represent the data in this model,
Relational databases Based on the relational model, it is typically presented in tables that have some relation to each other among which tables can be joined, and include indices.
Application DB The benefit of relational DBs is that they can easily be incorporated into multiple applications since they have defined schemas for the tables and a universal interface.</description></item><item><title>Introduction</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/introduction/</link><pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/introduction/</guid><description>Web scale scale data management is the management of large scale datasets with the goal of how to efficiently store, query, and run large amounts of transactions on the datasets.
Multiple different systems exist, such as P2P systems or cloud storage.</description></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/info/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4331-web-scale-data-management/info/</guid><description>IN4331 Web-Scale Data Management notes at TUDelft (Q4).
Studyguide: https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55287
The content is based on the lecture material/slides and summarized by myself.</description></item><item><title>Big Data and Distributed ML</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/bigdata/</link><pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/bigdata/</guid><description>Three different models of parallel computing:
Shared memory with multiple CPUs connected to a cache-coherent interconnect to multiple RAM modules, to which all have access. Distributed memory with multiple CPUs all being connected to their own RAM module and a non-cache-coherent interconnect. OpenMP where data is used where the computation is, which is the interface for multi-plaotform shared memory multiprocessing. Big Data First Generation Distributed File Systems The first open-source system is HDFS, which is a block based file system with built in replication and metadata journaling.</description></item><item><title>Middleware</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/middleware/</link><pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/middleware/</guid><description>Middleware is the part that goes between the OS (or lower level parts) and the applications, these define a set of protocols and APIs between the applications and the network or OS for services and communication. Java RMI is an example of this. Three levels of middleware exist:
Basic messaging Programming primitives (such as RMI) Full services (platform) There are different classes of middleware; the remote-call oriented (RPC), the object-oriented (RMI), the service-oriented (CORBA, web services), and the message-oriented (pub/sub, message bus).</description></item><item><title>Programming Models and System Architecture</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/systemsarchtecture/</link><pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/systemsarchtecture/</guid><description>Design Elements of a Distributed System The Architecture, is it client-server, hierarchical, peer-to-peer, etc. The programming model that is used, is it remote procedure call (RPC), library based, actor model based (akka), mapreduce, etc. Tradeoffs between the different programming modles and the sotware architecture have to be made, based on the functional/non-functional requirements, the cost of convenience, which is easier to use based on the hardware for example, and which is needed for a large scale system.</description></item><item><title>Non-Functional Requirements</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/non-functional/</link><pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/non-functional/</guid><description>Consistency Client vs. data centric consistency, in the client side model there is a consistency model for every client but not a global consistency. From the perspective of the client there is no difference in the two models. When to use which model is a tradeoff decision again, therefore when you can afford it you want to use a weaker consistency model. The client consistency can be used if there is some notion of a client session.</description></item><item><title>Resource Management and Scheduling</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/management/</link><pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/management/</guid><description>Reading Material Before Lecture Google Borg Google Borg is a management system for clusters that run several hundreds of thousands of jobs from many different applications, on many different clusters with each having several thousands machines.
Resource Management in Cloud Clouds contain large quantities of shared resources that provide these to large pools of data and computing with a variety of different interfaces. An important part of cloud computing is resource management, as there only are finite resources available.</description></item><item><title>Functional Requirements</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/functionalrequirements/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/functionalrequirements/</guid><description>Reading Material Before Lecture Four Types of Architecture Layered: where each part is stacked on top of each other and communicates down and up the stack only (e.g. TCP/IP stack) Object: where each object can communicate with other objects using remote method calls. Event based: where there is one message bus that connects the different components and each component can then publish events on the message bus. Shared data: where there is some shared (persistent) data space to which components can publish data or request data from.</description></item><item><title>Introduction</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/introduction/</link><pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/introduction/</guid><description>What is the difference between parallel computing and distributed computing?
In parallel computing there typically is one job that is split into multiple tasks, which are then split up to different cores/threads that run simultaneously. Such systems typically are homogeneous, the hardware is all the same, i.e. the threads run in the same system on the same CPU.
In distributed computing there are also multiple tasks, but these can come from one job or possibly multiple jobs.</description></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/info/</link><pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4391-distributed-systems-notes/info/</guid><description>IN4391 Distributed Systems notes at TUDelft (Q3).
Studyguide: https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55274 Book: Andrew S. Tanenbaum, Maarten Van Steen, Distributed Systems, Principles and Paradigms (2nd Edition), Prentice Hall, 2006.
The content is based on the lecture material and summarized by myself, due to licensing no actual content of the course is copied here and any images and phrases used come from the book.</description></item><item><title>State Machine Replication</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/smr/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/smr/</guid><description>The state of a machine is constituted by the contents of the data store (data written by the clients), and for redundancy might want to be replicated across multiple machines, such that if one fails another can immediately take over. In order for all machines to maintain the same state they have to implement linearizability, since they all need to execute the client requests in the exact same order.
Stopping Failure Tolerant SMR Algorithms Lamport&amp;rsquo;s Single-Value Paxos Consensus Algorithm The algorithm assumes only stopping failures (may restart later, mut no malicious or bugs(Byzantine fault)) and an asynchronous system.</description></item><item><title>Stabilization</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/stabilization/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/stabilization/</guid><description>For dealing with transient faults, which means that components in a distributed system may only fail temporarily, stabilizing algorithms have been devised that bring a system back into a correct sate from any incorrect sate it may be in.
A predicate, which is the defined order of saying what functions correctly and incorrectly and is defined as legal and illegal configurations, is called stable if once it holds and no faults occur, it continues to hold.</description></item><item><title>Consensus</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/consensus/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/consensus/</guid><description>Distributed systems will have to deal with faults caused by software or hardware components, not operating according to specification. Faults can be classified as permanent, once a processor exhibits a fault it will be considered as faulty for ever, which also leads to crash failures and malicious or Byzantine failures. Transient faults are when a processor may exhibit a fault but will return to correct operation again, i.e. transmission error or short power loss.</description></item><item><title>Minimum-Weight Spanning Trees</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/mst/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/mst/</guid><description>A reason for using minimum-weight spanning trees (MST) is to broadcast some message along the tree with minimum weight, if for example the transmission cost is used as the weight for edges. MSTs are constructed from weighted (unique weights) undirected graphs. Unique weights are required for finding a unique solution for an MST, and for the following algorithm. A fragment of graph $G$ is a subtree of its MST. Edge $e$ of $G$ is the minimum-weight outgoing edge (MOE) of fragment $F$ if it has the minimum weight that connects $F$ to another fragment.</description></item><item><title>Traversal Algorithms</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/traversal/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/traversal/</guid><description>Information in a DS may need to be propagated from a single node to all other nodes and the originator may want to receive information back. In a spanning tree of an undirected network, which is a sub-network of the original network that contains all nodes of the original network and that has the structure of a tree (the sparsest type of sub-network that keeps the system connected), messages need to traverse the tree efficiently.</description></item><item><title>Election</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/election/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/election/</guid><description>It may sometimes be necessary in a DS to assign a certain process with a higher privilege, for which election is used. If each process has a unique integer id, the system is said to be non-anonymous and anonymous otherwise. In anonymous rings no deterministic solution exists to election, as processes need to randomly generate some id value (e.g. 64 bit number). Algorithms that work in a ring of an unknown size are called uniform.</description></item><item><title>Token Loss</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/tokenloss/</link><pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/tokenloss/</guid><description>Since token based systems require the presence of a single token, if a token is lost due to unreliable links, this has to be detected and a single new token has to be generated n a unidirectional ring.
It works by using two tokens $t_0$ and $t_1$ that detect the loss of each other. The token message that transfers the token contains a token number that is the index of the token and a counter which is equal to plus or minus the number of times the tokens have met, plus if the token id is 0 and minus if it is 1.</description></item><item><title>Mutual Exclusion</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/mutualexclusion/</link><pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/mutualexclusion/</guid><description>In a distributed system in order to access the resource, a process will execute the critical section of the resource, hence it is required to guarantee that at most one process is in the critical section at a time. These algorithms aim to solve mutual exclusion with no deadlocks, no starvation, and some notion of fairness.
Assertion-Based Mutual Exclusion A process has to request permission from all or part of the other processes and based on their replies it may conclude that is the only one with rights to enter its CS.</description></item><item><title>Deadlock Detection</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/deadlockdetection/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/deadlockdetection/</guid><description>Models for Deadlock A (directed) Wait-For-Graph (WFG) can be maintained with the processes as nodes and with an edge from process $P$ to process $Q$ when $Q$ is holding a resource that $P$ is requesting.
In the resource model resources are associated with a process (e.g. $R_a$) and processes can then request said resource. When a process $P$ wants access to a resource $a$ from $R_a$, an edge is created from $P$ to $R_a$, and when the resource is granted, the edge is removed and replaced by an edge from $R_a$ to $P$ indicating that $R_a$ is waiting for $P$ to release the resource.</description></item><item><title>Lecture 12: Secret Sharing</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-12/</link><pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-12/</guid><description>Secret Sharing (Chapter 19) Threshold Cryptography split key into shares and a certain number of then can reconstruct the secret key. Or performing operations jointly (decryption, signing, etc.) but the secret key is not known to anyone (e.g. group signatures).
Dealer $(D)$ has the secret key $s$ and provides the shares to different parties $P_1,&amp;hellip;,P_n$
Distribution: Protocol in which the dealer provides each party $P_i$ a share $s_i$.
Reconstruction: Protocol in which a qualified set of parties pool their shares to obtain secret $s$.</description></item><item><title>Termination Detection</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/terminationdetection/</link><pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/terminationdetection/</guid><description>Termination detection is the problem of determining whether a distributed computation in a distributed system consisting of processes which communicate by means of message, has terminated.
Distributed computations have the following properties:
A process is either active or passive Only active processes can send messages An active process may become passive spontaneously A passive process becomes active at the reception of a message Termination Detection in an Asynchronous Unidirectional Ring with FIFO Communication There exits a process $P_{0}$ that is on top of the ring and other processes are connected to it in the order of $P_{n-1}$ to the right of $P_0$ and $P_1$ to the left.</description></item><item><title>Global States</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/globalstates/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/globalstates/</guid><description>Detecting of global states is the recording of an asynchronous system at some point in time for checkpointing or detecting stable properties such as deadlock or termination.
A cut presents a set of internal events and can be considered consistent (when receiving events happens after sending events) or inconsistent (when receiving events happen without the send events).
Chandy&amp;rsquo;s and Lamport&amp;rsquo;s algorithm for detecting global states in distributed systems with unidirectional FIFO channels Any processor wishing to record the global state of the system first records its own local state and then sends a marker on every outgoing channel.</description></item><item><title>Lecture 11: Certificates, Key Transport, and Key Agreement</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-11/</link><pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-11/</guid><description>Certificates, Key Transport, and Key Agreement (Chapter 18) What is key management?
Key generation Key distribution Key storage Key change Key usage Key destruction Requirements of key generation:
secret unpredictable strong key It is often desirable to frequently change the key in a cryptographic system.
Types of keys:
Static (or Long Term) Keys: few hours to a few years Ephemeral or Session (or Short Term) Keys a few seconds or a day Certificates and Certificate Authority We need a binding, linking a public key to an entity.</description></item><item><title>Lecture 10: Public Key Encryption and Signature Algorithms</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-10/</link><pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-10/</guid><description>Public Key Encryption and Signature Algorithms (Chapter 16) Passively Secure Encryption Schemes Goldwasser Micali Encryption Is based on the QUADRES problem, stating that given a composite integer $N$ and an integer $e$, it is hard to test if $a$ is a quadratic residue or not. This is given as checking if a value from the set $J_N$ of values that produce a jacobi symbol of 1 (hence might be quadratic residue) is in the set of real quadratic residues $Q_N$.</description></item><item><title>Message Ordering</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/messageodering/</link><pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/messageodering/</guid><description>In asynchronous systems messages may have arbitrary but finite delays but the applications may impose some kind of ordering on the messages (causal order). Multicasting is when a message is sent to group of processes rather than just to a single process.
Ordering A message order is causal when for every two messages $m_1$ and $m_2$, if $m(m_1)\rightarrow m(m_2)$ then $d_i(d_1)\rightarrow d_i(d_2)$ for all $i\in Dest(m_1)\cap Dest(m_2)$.
A message order is total when for every two messages $m_1$ and $m_2$, $d_i(m_1)\rightarrow d_i(m_2)$ iff $d_j(m_1)\rightarrow d_j(m_2)$ for all $i,j \in Dest(m_1) \cap Dest(m_1)$.</description></item><item><title>Lecture 9: The RSA Algorithm</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-9/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-9/</guid><description>The RSA Algorithm (Chapter 15) The RSA algorithm is based on the difficulty of the RSA problem that it is difficult to to find $d$ given a large composite number $N$ and $e$. It works by taking two large secret prime numbers $p$ and $q$ and computing $N=p*q$. Then picking an encryption exponent $e$ that satisfies
$$\text{gcd}(e,(p-1)*(q-1))=1$$
It is common to chose values $e=3,17,65537$. Now the public key will be shared as the pair of $\mathfrak{pC}=(N,e)$.</description></item><item><title>Synchronizers</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/synchronizers/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/synchronizers/</guid><description>Synchronizers are algorithms that simulate synchronous systems on top of asynchronous systems. They proceed in rounds of sending messages, receiving messages, and performing local computations, and these algorithms work on the foundation of issuing a pulse (clock) to allow a process to move to the next round.
Types of Synchronizers Alpha Synchronizer In $\alpha$-synchronizers, when a node receives a message, it sends an ACK message back to the sender. When a process received an ACK for every message it has sent in some round, it is called safe.</description></item><item><title>Lecture 8: Number Theory and Elliptic Curves</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-8/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-8/</guid><description>Number Theory and Elliptic Curves (Chapter 1 &amp;amp; 4) Modular Arithmetic A positive integer $N$ called modulus is written as
$$a=b\text{ mod }N$$
if $N$ divides $b-a$, $a$ and $b$ are congruent modulo $N$.
The set of values produced by postfix operation mod $N$ is $\mathbb{Z}_N=\{0,1,&amp;hellip;,N-1\}$ (can also be written as $\mathbb{Z}/N\mathbb{Z}$).
The properties of modulo arithmetic are
A group is a set which is closed, has an identity, is associative, and every element has an inverse.</description></item><item><title>Time Concepts</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/timeconcepts/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/timeconcepts/</guid><description>Time has an important role in computer systems, the applications that they run need to be able to keep track of time and events and compare timestamps. In asynchronous systems it may be necessary to reason about events based on their order of occurrence.
Happened-Before Relation The basis of the theory of ordering events in distributed systems is the happened-before relation, and is given as $\rightarrow$ as the following
Local Order: If $a,b\in E_i$ for $i$ and $a$ occurred in $P_i$ before $b$ then $a\rightarrow b$.</description></item><item><title>Lecture 7: Hash Functions, MAC, and Key Derivation Functions</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-7/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-7/</guid><description>Hash Functions, MAC, and Key Derivation Functions (Chapter 14) Hash functions are used for integrity protection (checksum, file system integrity like Bit-torrent), one way encryption (password protection), asymmetric crypto schemes, MACs, key derivations, pseudo random number generators, and many more applications.
Hash functions receive an arbitrary length bit string and output a fixed length string called the hash value, digest, fingerprint, or hashcode. Hash functions are one way: easy to compute $y$ given $x$, where $y: H: \{0-1\}*\rightarrow\{0-1\}^n$</description></item><item><title>Modeling Distributed Systems</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/modelingds/</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/modelingds/</guid><description>DSs and DAs are modelled with a set of processors or processes, which do local computations and send and receive messages, and are connected by unidirectional communication channels, and networks are assumed to be connected such that there is a path from every process to every other process.
In a complete network there is a link from every processor to every other processor. In ring every processor is connected to two other processors.</description></item><item><title>Introduction</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/introduction/</link><pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/introduction/</guid><description>What is a Distributed System? Distributed computer systems are collections of computer systems that present themselves as single entities to their users and are characterized by
Autonomy: The components of the DS have a certain power or authority to make their own decisions. Cooperation: The components of a DS are working together towards common goals. Communication: The components of the DS exchange information. Properties of Distributed Systems There is no regular structure such that a DS may be connected by heterogenous network technologies and consist of many different processors.</description></item><item><title>Lecture 6: Block Ciphers and Modes of Operation</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-6/</link><pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-6/</guid><description>Block Ciphers and Modes of Operation (Chapter 13) Block ciphers operate on blocks of plaintext at a time to produce blocks of ciphertext. Block sizes tend to be large, 64 bits in DES and 128 bits in other modern block ciphers. In order to limit the advantage of the adversary, the key space is kept very large such that $Adv_{{F_k}K}^{PRP}(A)\approx 1/|K|$. Block cipher is part of an encryption but requires a mode of operation, these are both developed independently.</description></item><item><title>Lecture 5: Modern Stream Ciphers</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-5/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-5/</guid><description>Modern Stream Ciphers (Chapter 12) Encrypt bits rather than blocks as this is faster and easier to implement in hardware and software. Stream ciphers work by
$$c=m\oplus F_k(0)$$
and are IND-PASS secure if the PRF is secure.
Feedback shift registers are a standard way of producing a binary stream of data. These consist of a number of memory cells, where some of them are tapped that feed a feedback function. Registers are shifted down and the result from the feedback is shifted to the empty cell.</description></item><item><title>Lecture 4: Defining Security</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-4/</link><pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-4/</guid><description>Security Games (Chapter 2.2) Security games are used to define security for cryptographic components, which contain an adversary and a challenger, and the idea is that the adversary needs to reach a certain objective given data provided by the challenger. These are typically represented visually with the adversary in a box and the challenger outside with the data it provides. The advantage of an adversary $$A$$ is defined as a function bounded by time $$t$$ that the adversary spends to try to solve the problem.</description></item><item><title>Lecture 3: Information Theoretic Security</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-3/</link><pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-3/</guid><description>Information Theoretic Security (Chapter 9) A crypto system is said to be computationally secure if the best possible algorithm for breaking it requires $$N$$ operations, where $$N$$ is some large number above $2^{128}$. It is impossible to prove that a system is computationally secure, as we do not know if there is a better algorithm for breaking it, hence a system is called secure if the best known algorithm requires a large number of computations.</description></item><item><title>Lecture 2: Classical Systems</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-2/</link><pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-2/</guid><description>Classical Systems (Chapter 7) An encryption algorithm or cipher is the transformation of plaintext to ciphertext with a secret key, depicted as
$$c=e_k(m)$$
with $m$ being the plaintext,
$e$ being the cipher function,
$k$ being the secret key,
$c$ being the ciphertext
The reverse process is called decryption or decipherment, depicted as
$$m=d_k(c)$$
The key above needs to be known to both parties, but must be kept secret from all others.</description></item><item><title>Lecture 1: Introduction</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-1/</link><pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/lecture-1/</guid><description>What is Security? Security means:
Confidentiality: Access to systems or data is limited to authorized parties.
Integrity: When you receive data, you get the &amp;ldquo;right&amp;rdquo; data.
Availability: The system or data is there when you want it.
Objectives of Cryptography - Message authentication: valid message? - Data origin authentication: valid sender? - Entity authentication: authenticate each other for communication Kerckhoff&amp;rsquo;s principle: The adversary knows all details about the crypto system except the private key.</description></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/info/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4150-distributed-algorithms-notes/info/</guid><description>IN4150 Distributed Algorithms notes at TUDelft (Q2).
Studyguide: https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55229
All content and images are based on and retrieved from the slides and the provided lecture notes for the course.</description></item><item><title>Info</title><link>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/info/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/notes/in4191-security-and-cryptography-notes/info/</guid><description>IN4191 Security and Cryptography notes at TUDelft (Q1).
Studyguide: https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=55221
Book: Nigel P. Smart. 2015. Cryptography Made Simple (1st. ed.). Springer Publishing Company, Incorporated.
All content and images are based on and retrieved from the book and the lecture slides.</description></item><item><title>Amsterdam Data Science Thesis Award</title><link>https://nicktehrany.github.io/awards/adsthesis/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/awards/adsthesis/</guid><description>For my BSc Computer Science Thesis I received the Inaugural Amsterdam Data Science Thesis award. The thesis was completed in 2020 at the Vrije Universiteit Amsterdam with @Large Massivizing Computer Systems research group. The full thesis is available here and more information on the Amsterdam Data Science award can be found here.</description></item><item><title>textemp</title><link>https://nicktehrany.github.io/projects/completed-projects/textemp/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/projects/completed-projects/textemp/</guid><description>I was tired of having to create new folder structure and latex files every time I had to create a new latex project (which was quite often). Therefore I made a quick and simple command to set up the basic directory structure and initialize the .tex files with the most basic necessities.
Setup Clone the repo anywhere you like,
git clone https://github.com/nicktehrany/textemp Source the textemp script in your .bashrc, .zshrc or other .</description></item><item><title>BSc Computer Science Thesis</title><link>https://nicktehrany.github.io/publications/theses/bsc-computer-science-thesis/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/publications/theses/bsc-computer-science-thesis/</guid><description>&amp;ldquo;Evaluating Performance Characteristics of the PMDK Persistent Memory Software Stack&amp;rdquo; Abstract: Society nowadays revolves more and more around data. Data Science, Machine Learning, and Artificial Intelligence depend on large amounts of data to conduct research, and train machine learning models and agents. With the ever increasing amount of data, comes the need for faster storage. The quest for new storage devices has resulted in the development of non-volatile memories, that run alongside conventional memory and are directly accessible by the CPU, with large capacity of persistent storage.</description></item><item><title>Dotfiles</title><link>https://nicktehrany.github.io/projects/active-projects/dotfiles/</link><pubDate>Sat, 11 Jan 2020 00:00:00 +0000</pubDate><guid>https://nicktehrany.github.io/projects/active-projects/dotfiles/</guid><description>Visuals The current overall look for my shell and other setup (shell theme and so forth can also all be seen in the neofetch output).
Installation I use dotbot for installing and linking all files.
git clone https://github.com/nicktehrany/dotfiles cd dotfiles # For installing the workstation profile (check meta/profiles/) sudo ./install-profile workstation Note all previously linked files will be overwritten, check the meta/configs/ for symlinks that will be created.
Check the configs from the workstation profile to see which configs will be installed, which can then be found in the meta/configs/ directory.</description></item></channel></rss>